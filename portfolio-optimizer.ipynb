{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#These are the libraries you can use.  You may add any libraries directy related to threading if this is a direction\n",
    "#you wish to go (this is not from the course, so it's entirely on you if you wish to use threading).  Any\n",
    "#further libraries you wish to use you must email me, james@uwaterloo.ca, for permission.\n",
    "\n",
    "from IPython.display import display, Math, Latex\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numpy_financial as npf\n",
    "import yfinance as yf\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "import scipy as sp \n",
    "from scipy.optimize import minimize\n",
    "from threading import Thread"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Portfolio Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV Ticker file path\n",
    "csv_file_path = \"Tickers.csv\"\n",
    "\n",
    "# Date ranges used to analyze the percent returns of stocks\n",
    "returns_start = \"2024-11-14\" \n",
    "returns_end = \"2025-11-14\" \n",
    "\n",
    "# Date ranges used to determine volume \n",
    "vol_start = \"2024-10-01\"\n",
    "vol_end = \"2025-10-01\"\n",
    "\n",
    "# Volume filtering\n",
    "min_avg_vol = 5000\n",
    "min_trading_days_per_month = 18\n",
    "\n",
    "# Portfolio size constraints\n",
    "min_port_size = 15\n",
    "max_port_size = 25\n",
    "\n",
    "# Market cap constraints (in USD)\n",
    "small_cap_threshold = 2_000_000_000\n",
    "large_cap_threshold = 10_000_000_000\n",
    "\n",
    "# Weight constraints (per stock)\n",
    "min_weight = None # Later will be set to the formula: (100/(2*num_assets))/100\n",
    "max_weight = 0.15\n",
    "\n",
    "# Sector constraints\n",
    "max_sector_weight = 0.40\n",
    "\n",
    "# Currency constraints\n",
    "allowed_currencies = [\"CAD\", \"USD\"]\n",
    "\n",
    "# Budget\n",
    "total_budget = 1_000_000\n",
    "\n",
    "# Fee structure\n",
    "flat_fee_cad = 2.5\n",
    "variable_fee_per_share = 0.001\n",
    "\n",
    "# Max iterations when trying to create a succesful portfolio\n",
    "max_iterations = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def last_trading_day(date=None):\n",
    "    \"\"\"\n",
    "    Find the last trading day (weekday) before a given date\n",
    "    \"\"\"\n",
    "    if date is None:\n",
    "        date = datetime.today()\n",
    "    # Step back one day at a time until it's a weekday, aka Monday or Friday\n",
    "    while date.weekday() >= 5: # Saturday = 5, Sunday = 6\n",
    "        date -= timedelta(days=1)\n",
    "    return date\n",
    "\n",
    "purchase_date = last_trading_day()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_tickers(csv_file_path):\n",
    "   \"\"\"\n",
    "   valid_tickers is the prelimary checker, it reads a csv and then removes any tickers that trade less than the minimum volume\n",
    "\n",
    "   :param csv_file_path: A string that represents the csv file name that will be read\n",
    "   :return: A list of names of the valid tickers\n",
    "   \"\"\"\n",
    "   \n",
    "   print(f\"Now loading tickers from {csv_file_path}.\")\n",
    "\n",
    "   # Scrape for unique tickers\n",
    "   try:\n",
    "      tickers_df = pd.read_csv(csv_file_path, header=None)\n",
    "      tickers_df.columns = [\"Ticker\"]\n",
    "      tickers_df[\"Ticker\"] = tickers_df[\"Ticker\"].str.strip()\n",
    "      tickers_df = tickers_df.drop_duplicates()\n",
    "      ticker_list = tickers_df[\"Ticker\"].tolist()\n",
    "\n",
    "      print(f\"{len(ticker_list)} unique tickers have been found in the csv. Now downloading tickers.\")\n",
    "   except FileNotFoundError:\n",
    "      raise FileNotFoundError(f\"Could not find file path to {csv_file_path}.\")\n",
    "\n",
    "   # Bulk download the ticker information\n",
    "   hist = yf.download(\n",
    "       tickers=ticker_list,\n",
    "       start=vol_start,\n",
    "       end=vol_end,\n",
    "       group_by=\"ticker\",\n",
    "       auto_adjust=False,\n",
    "       threads=True,\n",
    "       progress=False\n",
    "    )\n",
    "   \n",
    "   if hist.empty:\n",
    "      raise ValueError(\"No data downloaded, check ticker symbols and date range.\")\n",
    "\n",
    "   # Count the rows per month per ticker\n",
    "   trading_days_per_month = hist.groupby(hist.index.to_period(\"M\")).transform(\"count\")\n",
    "\n",
    "   # Keep only rows where all tickers in that month exceed configured minimum\n",
    "   valid_days_mask = trading_days_per_month >= min_trading_days_per_month\n",
    "   hist_filtered = hist[valid_days_mask]\n",
    "\n",
    "   # Remove the months that do not meet threshold and any tickers that don't have a large enough trading volume\n",
    "   vol_filtered = hist_filtered.xs(\"Volume\", axis=1, level=1)\n",
    "   avg_vol_filtered = vol_filtered.mean()\n",
    "   valid_tickers = avg_vol_filtered[avg_vol_filtered >= min_avg_vol].index.tolist()\n",
    "\n",
    "   return valid_tickers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Begin threading set up\n",
    "def get_format_info(ticker, ticker_data):\n",
    "   \"\"\"\n",
    "   get_format_info is the threading worker function, it is what will be done in parallel. It finds wanted ticker info\n",
    "   and adds it to a list\n",
    "\n",
    "   :param ticker: The ticker whose info is wanted\n",
    "   :param ticker_data: A list that keeps track of the ticker info thats been extracted\n",
    "   \"\"\"\n",
    "   try:\n",
    "      ticker_info = yf.Ticker(ticker).info\n",
    "\n",
    "      # Check currency\n",
    "      currency = ticker_info.get(\"currency\")\n",
    "      if (currency not in allowed_currencies):\n",
    "         return\n",
    "      \n",
    "      # Check if its an equity\n",
    "      if ticker_info.get(\"quoteType\") != \"EQUITY\": \n",
    "         return\n",
    "      \n",
    "      ticker_data.append((\n",
    "            ticker,\n",
    "            ticker_info.get(\"sector\"),\n",
    "            currency,\n",
    "            ticker_info.get(\"marketCap\")))\n",
    "      \n",
    "   except:\n",
    "      pass\n",
    "\n",
    "#Threading occurs\n",
    "def format_tickers(csv_file_path):\n",
    "   \"\"\"\n",
    "   format_tickers is the main function that performs threading onto the worker function and gets the dataframe of tickers and wanted info\n",
    "\n",
    "   :param csv_file_path: A string that represents the csv file name that will be read\n",
    "   :return: A Dataframe that contains all the info we want for every valid ticker\n",
    "   \"\"\"\n",
    "   ticker_list = valid_tickers(csv_file_path)\n",
    "\n",
    "   print(f\"Fetching info for {len(ticker_list)} tickers.\")\n",
    "   \n",
    "   # Threading for parallel API calls\n",
    "   ticker_data = []\n",
    "   threads = []\n",
    "\n",
    "   # Loop and create threads of each ticker in the ticker_list\n",
    "   for t in ticker_list:\n",
    "      thread = Thread(target=get_format_info, args=(t, ticker_data))\n",
    "      threads.append(thread)\n",
    "      thread.start()\n",
    "\n",
    "   # Once all threads complete, join them together \n",
    "   for th in threads:\n",
    "      th.join()\n",
    "\n",
    "   # Create and clean up the Dataframe\n",
    "   ticker_df = pd.DataFrame(ticker_data, columns=[\"Ticker\",\"Sector\",\"Currency\",\"MarketCap\"]) \n",
    "   ticker_df = ticker_df.dropna(subset=[\"Sector\", \"MarketCap\"])\n",
    "   ticker_df = ticker_df.reset_index(drop=True)\n",
    "\n",
    "   print(f\"{len(ticker_df)} tickers with complete information found.\")\n",
    "\n",
    "   return ticker_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gets weekly closes of all the stocks in a list of tickers\n",
    "def get_weekly_closes (ticker_list):\n",
    "    \"\"\"     \n",
    "    get_weekly_closes performs data extraction to get weekly closing price info of stocks\n",
    "\n",
    "    :param ticker_lst: List that holds all tickers\n",
    "    :return: returns a Dataframe that holds the weekly closing price of each ticker on every week\n",
    "    \"\"\"\n",
    "    print(f\"Downloading price data for {len(ticker_list)} tickers.\")\n",
    "\n",
    "    price_hist = yf.download(\n",
    "        tickers=ticker_list,\n",
    "        start=returns_start,\n",
    "        end=returns_end,\n",
    "        group_by=\"ticker\",\n",
    "        auto_adjust=True,\n",
    "        threads=True,\n",
    "        progress=False,\n",
    "        multi_level_index=False\n",
    "    )\n",
    "\n",
    "    if price_hist.empty:\n",
    "        raise ValueError(\"No data downloaded, check date range and tickers.\")\n",
    "    \n",
    "    # yf.download handles single ticker and multiple ticker download cases differently\n",
    "    if len(ticker_list) == 1:\n",
    "        close_data = price_hist[[\"Close\"]]\n",
    "        close_data.columns = ticker_list\n",
    "    else:\n",
    "        close_data = price_hist.xs('Close', axis=1, level=1)\n",
    "\n",
    "    # Get the weekly closing price, and then clean the index\n",
    "    weekly_closes = close_data.resample(\"W-FRI\").last()\n",
    "    weekly_closes.index = weekly_closes.index.strftime('%Y-%m-%d')\n",
    "\n",
    "    print(f\"Completed download of {len(weekly_closes)} tickers.\")\n",
    "\n",
    "    return weekly_closes\n",
    "\n",
    "# Creates a df with the (weekly) %change for each column\n",
    "def get_percent_change (closes):\n",
    "    \"\"\"     \n",
    "    get_percent_change performs calculations that determines the percent change of closing prices\n",
    "\n",
    "    :param closes: Dataframe that holds closing prices\n",
    "    :return: returns a Dataframe that holds percent changes of closing prices\n",
    "    \"\"\"\n",
    "\n",
    "    percent_change = closes.pct_change(fill_method=None)\n",
    "\n",
    "    return percent_change\n",
    "\n",
    "# Calculate covariance, correlation, variance, standard deviation\n",
    "def get_calculations(ticker_list, start_date, end_date):\n",
    "    \"\"\"     \n",
    "    get_calculations performs calculations for covariance, correlation, variance, standard deviation\n",
    "\n",
    "    :param ticker_list: list of all tickers\n",
    "    :param start_date: start date for calculations\n",
    "    :param end_date: end date for calculations\n",
    "    :return: returns a dictionary that holds covariance, correlation, standard deviation, and variance \n",
    "    \"\"\"\n",
    "    weekly_closes = get_weekly_closes(ticker_list)\n",
    "    weekly_percent_change = get_percent_change(weekly_closes)\n",
    "\n",
    "    covariance_matrix = {\n",
    "        'Covariance': weekly_percent_change.cov(),\n",
    "        'Correlation': weekly_percent_change.corr(),\n",
    "        'Variance': weekly_percent_change.var(),\n",
    "        'Std_Dev': weekly_percent_change.std(),\n",
    "        'Returns': weekly_percent_change}\n",
    "    \n",
    "    return covariance_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the function we want to minimize, aka the minimum variance function\n",
    "def port_variance(weights, cov_matrix):\n",
    "    \"\"\"     \n",
    "    port_variance is the function that calculates the variance of a portfolio. It performs \n",
    "    dot product/matrix multiplication on the weights and covariance matrixes. \n",
    "\n",
    "    :param weights: an array that represents the weight of each asset\n",
    "    :param cov_matrix: a 2D matrix that represents the covariance between each asset \n",
    "    :return: variance of the portfolio\n",
    "    \"\"\"\n",
    "    weights_col = weights.reshape(-1, 1) # Turns into column vector\n",
    "    port_var = np.dot(weights_col.transpose(), (np.dot(cov_matrix, weights_col))) # Doing dot product \n",
    "    return port_var[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primary minimization, there are no bounds in this \n",
    "def unbounded_optimization(cov_matrix):\n",
    "    \"\"\"     \n",
    "    unbounded_optimization finds the weightings that result in the mimimum variance without considering any constraints. \n",
    "    This is the first-stage optimization. Used to create df that is used to create portfolios. \n",
    "\n",
    "    :param cov_matrix: a 2D matrix that represents the covariance between each asset \n",
    "    :return: returns the minimum variance and the weightings associated with that\n",
    "    \"\"\"\n",
    "    num_assets = cov_matrix.shape[0]\n",
    "    initial_weight = (1/num_assets) * num_assets # The initial guess of the weights\n",
    "\n",
    "    constraint = {\n",
    "        'type':'eq', # Constraint type is equality\n",
    "        'fun': lambda w: sum(w) - 1 # The function's weight's must sum to 1\n",
    "        }\n",
    "    \n",
    "    weight_bounds = [(0, 1)] * num_assets # Does not allow short selling\n",
    "    \n",
    "    # Finds the resilt of the minimization of the port_variance function, using the initial guess, keeping the cov_matrix constant using the SLSQP method, and with the above listed constraint\n",
    "    result = minimize(fun=port_variance, x0=initial_weight, args=(cov_matrix,), method='SLSQP', bounds=weight_bounds, constraints=constraint)\n",
    "    \n",
    "    return result.fun, result.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The secondary optimization that includes the bounds \n",
    "def bounded_optimization(cov_matrix, min_weight, max_weight):\n",
    "    \"\"\"     \n",
    "    bounded_optimization finds the weightings that result in the mimimum variance while considering the bounds.\n",
    "    This is the second-stage optimization.\n",
    "\n",
    "    :param cov_matrix: a 2D matrix that represents the covariance between each asset \n",
    "    :return: returns the minimum variance and the weightings associated with that\n",
    "    \"\"\"\n",
    "    num_assets = len(cov_matrix[0]) \n",
    "    initial_weight = [1/num_assets] * num_assets # The initial guess of the weights\n",
    "\n",
    "    constraint = {\n",
    "        'type':'eq', # Constraint type is equality\n",
    "        'fun': lambda w: sum(w) - 1 # The function's weight's must sum to 1\n",
    "        }\n",
    "    \n",
    "    weight_bounds = [(min_weight, max_weight)] * num_assets\n",
    "    \n",
    "    # Finds the resilt of the minimization of the port_variance function, using the initial guess, keeping the cov_matrix constant using the SLSQP method, and with the above listed constraint\n",
    "    result = minimize(fun=port_variance, x0=initial_weight, args=(cov_matrix,), method='SLSQP', bounds = weight_bounds, constraints=constraint)\n",
    "    \n",
    "    return result.fun, result.x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checks for small-cap and large-cap stock in portfolio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_cap_diversity(portfolio_df, all_tickers_df):\n",
    "    \"\"\"\n",
    "    Checks if a portfolio has both small and large cap stocks\n",
    "\n",
    "    :param portfolio_df: A dataframe of the current portfolio\n",
    "    :param all_tickers_df: Full ticker dataframe to find replacements, has info regarding market cap, sector, currency, etc, pre-sorted\n",
    "    :return: Fixed portfolio (or original if its already diversified)\n",
    "    \"\"\"\n",
    "\n",
    "    num_small = (portfolio_df['MarketCap'] < small_cap_threshold).sum()\n",
    "    num_large = (portfolio_df['MarketCap'] > large_cap_threshold).sum()\n",
    "\n",
    "    small_cap_stocks = all_tickers_df[(all_tickers_df[\"MarketCap\"] < small_cap_threshold) & (~all_tickers_df[\"Ticker\"].isin(portfolio_df[\"Ticker\"]))]\n",
    "    large_cap_stocks = all_tickers_df[(all_tickers_df[\"MarketCap\"] > large_cap_threshold) & (~all_tickers_df[\"Ticker\"].isin(portfolio_df[\"Ticker\"]))]\n",
    "    \n",
    "    # Passes cap diversity\n",
    "    if num_small >=1 and num_large >= 1:\n",
    "        print(\"Portfolio passes cap diversity.\")\n",
    "        return portfolio_df\n",
    "    \n",
    "    # Must add small cap\n",
    "    if num_small == 0:\n",
    "        print(\"Adding small cap stock.\")\n",
    "\n",
    "        if len(small_cap_stocks) == 0:\n",
    "            print(\"No small cap stocks availaible. Cannot ensure diversity, returning nothing.\")\n",
    "            return None\n",
    "        \n",
    "        if num_large > 1:\n",
    "            mid_large_mask = portfolio_df[\"MarketCap\"] > small_cap_threshold\n",
    "            replace_idx = portfolio_df[mid_large_mask].index[-1]\n",
    "        else:\n",
    "            mid_mask = (portfolio_df[\"MarketCap\"] > small_cap_threshold) & (portfolio_df[\"MarketCap\"] < large_cap_threshold)\n",
    "            replace_idx = portfolio_df[mid_mask].index[-1]\n",
    "\n",
    "        portfolio_df = portfolio_df.drop(replace_idx)\n",
    "        portfolio_df = pd.concat([portfolio_df, small_cap_stocks.head(1)], ignore_index=True)\n",
    "    \n",
    "    # Must add large cap\n",
    "    if num_large == 0:\n",
    "        print(\"Adding large cap stock\")\n",
    "\n",
    "        if len(large_cap_stocks) == 0:\n",
    "            print(\"No large cap stocks availaible. Cannot ensure diversity, returning nothing.\")\n",
    "            return None\n",
    "        \n",
    "        if num_small > 1:\n",
    "            mid_small_mask = portfolio_df[\"MarketCap\"] < large_cap_threshold\n",
    "            replace_idx = portfolio_df[mid_small_mask].index[-1]\n",
    "        else:\n",
    "            mid_mask = (portfolio_df[\"MarketCap\"] > small_cap_threshold) & (portfolio_df[\"MarketCap\"] < large_cap_threshold)\n",
    "            replace_idx = portfolio_df[mid_mask].index[-1]\n",
    "\n",
    "        portfolio_df = portfolio_df.drop(replace_idx)\n",
    "        portfolio_df = pd.concat([portfolio_df, large_cap_stocks.head(1)], ignore_index=True)\n",
    "\n",
    "    num_small_after = (portfolio_df['MarketCap'] < small_cap_threshold).sum()\n",
    "    num_large_after = (portfolio_df['MarketCap'] > large_cap_threshold).sum()\n",
    "    print(f\"There are now {num_small_after} small-cap and {num_large_after} large-cap stocks\")\n",
    "\n",
    "    return portfolio_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_sector_weights(tickers, weights, all_tickers_df):\n",
    "    \"\"\"\n",
    "    Calculate how much of the portfolio, by weight, belongs to each sector.\n",
    "\n",
    "    :param tickers: list of ticker names \n",
    "    :param weights: list of weights from the optimizer \n",
    "    :param all_tickers_df: dataframe that has info regarding market cap, sector, currency, etc, pre-sorted\n",
    "    :return: the sector weight total\n",
    "    \"\"\"\n",
    "\n",
    "    weight_df = pd.DataFrame({\n",
    "        \"Ticker\": tickers,\n",
    "        \"Weight\": weights\n",
    "    })\n",
    "\n",
    "    weight_df = weight_df.merge(all_tickers_df[[\"Ticker\", \"Sector\"]], on=\"Ticker\", how=\"left\")\n",
    "    sector_weights = (weight_df.groupby(\"Sector\"))[\"Weight\"].sum()\n",
    "\n",
    "    return sector_weights\n",
    "\n",
    "def get_overweight_sectors(tickers, weights, all_tickers_df):\n",
    "    \"\"\"\n",
    "    Finds any sector that exceeds the weight limit.\n",
    "\n",
    "    :param tickers: list of ticker names \n",
    "    :param weights: list of weights from the optimizer \n",
    "    :param all_tickers_df: dataframe that has info regarding market cap, sector, currency, etc, pre-sorted\n",
    "    :return: dictionary of overweight sectors and their weights, or an empty dictionary if all within limit\n",
    "    \"\"\"\n",
    "    sector_weights = compute_sector_weights(tickers, weights, all_tickers_df)\n",
    "\n",
    "    overweight = sector_weights[sector_weights > max_sector_weight]\n",
    "\n",
    "    return overweight.to_dict()\n",
    "\n",
    "def find_replacement_ticker(current_tickers, overweight_sectors, used_tickers, all_tickers_df):\n",
    "    \"\"\"\n",
    "    Finds a replacement ticker\n",
    "\n",
    "    :param current_tickers: current portfolio tickers\n",
    "    :param overweight_sectors: overweight sectors that should be avoided in our replacement\n",
    "    :param used_tickers: set of tickers that have been tried already\n",
    "    :param all_tickers_df: Full ticker dataframe to find replacements, has info regarding market cap, sector, currency, etc, pre-sorted\n",
    "    :return: Ticker symbol, or none if theres no valid replacements\n",
    "    \"\"\"\n",
    "\n",
    "    available_tickers = all_tickers_df[(~all_tickers_df[\"Ticker\"].isin(used_tickers)) & \n",
    "                                       (~all_tickers_df[\"Sector\"].isin(overweight_sectors)) &\n",
    "                                       (~all_tickers_df[\"Ticker\"].isin(current_tickers))]\n",
    "\n",
    "    if len(available_tickers) == 0:\n",
    "        return None\n",
    "    \n",
    "    return available_tickers.iloc[0]['Ticker']\n",
    "\n",
    "def optimize_with_sector_limits(initial_tickers, all_tickers_df):\n",
    "    \"\"\"\n",
    "    Optimize portfolio while respecting sector weight constraints and other constraints.\n",
    "\n",
    "    Algorithm:\n",
    "    1. Optimize weights for current portfolio\n",
    "    2. Check sector constraints of the portfolio\n",
    "    3. If violated, replace smallest-weight stock from overweight sector (make sure to protect small/large cap stocks)\n",
    "    4. Repeat until constraints satisfied or max iterations reached\n",
    "\n",
    "    :param initial_tickers: the initial tickers make up the first iteration of the current portfolio\n",
    "    :param all_tickers_df: dataframe that has info regarding market cap, sector, currency, etc, pre-sorted\n",
    "    :return: the list of tickers of the succesful portfolio, the variance of said portfolio, and its associated weights\n",
    "    \"\"\"\n",
    "\n",
    "    current_tickers = initial_tickers\n",
    "    used_tickers = initial_tickers\n",
    "\n",
    "    for iteration in range(max_iterations):\n",
    "        cov_matrix = get_calculations(current_tickers, returns_start, returns_end)['Covariance'].to_numpy()\n",
    "        num_assets = len(current_tickers)\n",
    "        min_weight = (100 / (2 * num_assets)) / 100\n",
    "        \n",
    "        # Conduct optimization and then create dataframe of the tickers with the weights, sector, and market caps aligned\n",
    "        variance, weights = bounded_optimization(cov_matrix, min_weight, max_weight)\n",
    "        portfolio_df = pd.DataFrame({\n",
    "            'Ticker': current_tickers,\n",
    "            'Weights': weights\n",
    "        })\n",
    "        portfolio_df = portfolio_df.merge(all_tickers_df[[\"Ticker\", \"Sector\", \"MarketCap\"]], on=\"Ticker\")\n",
    "\n",
    "        # Begin checking for sector constraints\n",
    "        overweight_info = get_overweight_sectors(current_tickers, weights, all_tickers_df)\n",
    "\n",
    "        # Optimization succesful case, all constraints have been satisfied. \n",
    "        if len(overweight_info) == 0:\n",
    "            print(f\"All constraints satisfied on iteration {iteration+1}. Final variance: {variance:.6f}\")\n",
    "            sector_weights = compute_sector_weights(current_tickers, weights, all_tickers_df)\n",
    "\n",
    "            print(\"Final sector distribution:\")\n",
    "            sector_df = pd.DataFrame({\n",
    "                'Sector': sector_weights.index,\n",
    "                'Weight': sector_weights.values\n",
    "            }).sort_values(\"Weight\", ascending=False)\n",
    "\n",
    "            display(sector_df)\n",
    "            \n",
    "            return current_tickers, variance, weights\n",
    "\n",
    "        # Constraints not satisfied yet, begin fixing them.\n",
    "\n",
    "        # Find worst sector and fix it.\n",
    "        worst_sector = max(overweight_info, overweight_info.get)\n",
    "\n",
    "        # Set up protected large/small caps. \n",
    "        current_portfolio = all_tickers_df[all_tickers_df['Ticker'].isin(current_tickers)].copy()\n",
    "        num_small = (current_portfolio['MarketCap'] < small_cap_threshold).sum()\n",
    "        num_large = (current_portfolio['MarketCap'] > large_cap_threshold).sum()\n",
    "\n",
    "        protected_tickers = []\n",
    "\n",
    "        if num_small == 1:\n",
    "            protected_tickers.append(current_portfolio[current_portfolio[\"MarketCap\"] < small_cap_threshold][\"Ticker\"].iloc[0])\n",
    "        \n",
    "        if num_large == 1:\n",
    "            protected_tickers.append(current_portfolio[current_portfolio[\"MarketCap\"] > large_cap_threshold][\"Ticker\"].iloc[0])\n",
    "        \n",
    "        # Find stocks in worst overweight sector that can be replaced\n",
    "        replaceable_df = portfolio_df[(portfolio_df[\"Sector\"] == worst_sector) & (~portfolio_df[\"Ticker\"].isin(protected_tickers))]\n",
    "\n",
    "        # Find least important ticker and its corresponding information\n",
    "        min_weight_row = replaceable_df.loc[replaceable_df['Weight'].idxmin()]\n",
    "        removed_ticker = min_weight_row[\"Ticker\"]\n",
    "        removed_idx = current_tickers.index(removed_ticker)\n",
    "\n",
    "        # Begin replacement algorithm\n",
    "        all_overweight_sectors = list(overweight_info)\n",
    "        replacement = find_replacement_ticker(current_tickers, all_overweight_sectors, used_tickers, all_tickers_df)\n",
    "\n",
    "        if replacement is None:\n",
    "            print(\"No replacement found outside overweight sectors\")\n",
    "            return None, None, None\n",
    "        \n",
    "        print(f\"Iteration {iteration + 1}: Replacing {removed_ticker} with {replacement}\")\n",
    "        current_portfolio[removed_idx] = replacement\n",
    "        used_tickers.add(replacement)\n",
    "\n",
    "        # Continue loop\n",
    "    \n",
    "    print(f\"{max_iterations} iterations reached. Terminated to prevent memory usage.\")\n",
    "    return None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a portfolio that follows the small/large cap constraints\n",
    "def create_portfolio(size, all_tickers_df):\n",
    "    \"\"\" \n",
    "    create_portfolio creates a portfolio of a given size that satisfies market cap diversity\n",
    "\n",
    "    :param size: the size of the portfolio\n",
    "    :param all_tickers_df: dataframe pre-sorted through first-stage optimization that has info regarding market cap, sector, currency, etc\n",
    "    :return: a list of tickers that are in the portfolio, or none if it cannot create a portfolio that satisfies all requirements\n",
    "    \"\"\"\n",
    "\n",
    "    if size > len(all_tickers_df):\n",
    "        print(f\"Cannot create portfolio of size {size} with only {len(all_tickers_df)} tickers\")\n",
    "        return None\n",
    "    \n",
    "    initial_portfolio_df = all_tickers_df.head(size).copy()\n",
    "\n",
    "    initial_portfolio_df = ensure_cap_diversity(initial_portfolio_df, all_tickers_df)\n",
    "    \n",
    "    return initial_portfolio_df['Ticker'].tolist()\n",
    "\n",
    "def find_optimized_portfolio(all_tickers_df):\n",
    "    \"\"\"\n",
    "    Loops through all possible portfolio sizes and returns the optimized one. \n",
    "\n",
    "    :param all_tickers_df: dataframe pre-sorted through first-stage optimization that has info regarding market cap, sector, currency, etc\n",
    "    :return: a list of the most optimal variance, its tickers, and the weightings associated\n",
    "    \"\"\"\n",
    "    all_ports = []\n",
    "    all_variance = []\n",
    "    all_weights = []\n",
    "    count = 0\n",
    "\n",
    "    # Go through all possible portfolio sizes and then find the one that results in minimum variance \n",
    "    for i in range(min_port_size, max_port_size + 1):\n",
    "        base_port_lst = create_portfolio(i, all_tickers_df)\n",
    "\n",
    "        if base_port_lst is None:\n",
    "            print(f\"Failed to build portfolio of size {i}, skipping.\") # portfolio requirements failed\n",
    "            count += 1\n",
    "            continue\n",
    "\n",
    "        final_port, final_var, final_w = optimize_with_sector_limits(base_port_lst, all_tickers_df)\n",
    "        if final_port is None:\n",
    "            print(f\"Sector constraint failed for portfolio size {count+10}, skipping.\") # portfolio requirements failed\n",
    "            count += 1\n",
    "            continue\n",
    "\n",
    "        all_ports.append(final_port)\n",
    "        all_variance.append(final_var)\n",
    "        all_weights.append(final_w)\n",
    "        count += 1\n",
    "    \n",
    "    if not all_variance:\n",
    "        print(\"No valid portfolios were generated. Please check ticker csv.\") \n",
    "    else:\n",
    "        smallest_var = min(all_variance)\n",
    "        index = all_variance.index(smallest_var)\n",
    "        target = [smallest_var, all_ports[index], all_weights[index]]\n",
    "        return target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now loading tickers from Tickers.csv.\n",
      "39 unique tickers have been found in the csv. Now downloading tickers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "6 Failed downloads:\n",
      "['SQ', 'HDFC.NS', 'INVALIDTIC', 'CELG', 'ASDFA.TO']: YFTzMissingError('possibly delisted; no timezone found')\n",
      "['XZO']: YFPricesMissingError('possibly delisted; no price data found  (1d 2024-10-01 -> 2025-10-01) (Yahoo error = \"Data doesn\\'t exist for startDate = 1727755200, endDate = 1759291200\")')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching info for 32 tickers.\n",
      "30 tickers with complete information found.\n",
      "Downloading price data for 30 tickers.\n",
      "Completed download of 53 tickers.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The number of bounds is not compatible with the length of `x0`.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yello\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\scipy\\optimize\\_minimize.py:1114\u001b[39m, in \u001b[36m_validate_bounds\u001b[39m\u001b[34m(bounds, x0, meth)\u001b[39m\n\u001b[32m   1113\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1114\u001b[39m     bounds.lb = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbroadcast_to\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbounds\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1115\u001b[39m     bounds.ub = np.broadcast_to(bounds.ub, x0.shape)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yello\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\numpy\\lib\\_stride_tricks_impl.py:410\u001b[39m, in \u001b[36mbroadcast_to\u001b[39m\u001b[34m(array, shape, subok)\u001b[39m\n\u001b[32m    369\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Broadcast an array to a new shape.\u001b[39;00m\n\u001b[32m    370\u001b[39m \n\u001b[32m    371\u001b[39m \u001b[33;03mParameters\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    408\u001b[39m \u001b[33;03m       [1, 2, 3]])\u001b[39;00m\n\u001b[32m    409\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m410\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_broadcast_to\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubok\u001b[49m\u001b[43m=\u001b[49m\u001b[43msubok\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreadonly\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yello\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\numpy\\lib\\_stride_tricks_impl.py:349\u001b[39m, in \u001b[36m_broadcast_to\u001b[39m\u001b[34m(array, shape, subok, readonly)\u001b[39m\n\u001b[32m    348\u001b[39m extras = []\n\u001b[32m--> \u001b[39m\u001b[32m349\u001b[39m it = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnditer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmulti_index\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mrefs_ok\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mzerosize_ok\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mextras\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m    \u001b[49m\u001b[43mop_flags\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mreadonly\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mitershape\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mC\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m it:\n\u001b[32m    353\u001b[39m     \u001b[38;5;66;03m# never really has writebackifcopy semantics\u001b[39;00m\n",
      "\u001b[31mValueError\u001b[39m: operands could not be broadcast together with remapped shapes [original->remapped]: (30,)  and requested shape (1,)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m valid_tickers_df = format_tickers(csv_file_path) \n\u001b[32m      4\u001b[39m covariance_matrix = ((get_calculations(valid_tickers_df[\u001b[33m\"\u001b[39m\u001b[33mTicker\u001b[39m\u001b[33m\"\u001b[39m].tolist(), returns_start, returns_end))[\u001b[33m'\u001b[39m\u001b[33mCovariance\u001b[39m\u001b[33m'\u001b[39m]).to_numpy()\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m unbound_var, unbound_weights = \u001b[43munbounded_optimization\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcovariance_matrix\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m valid_tickers_df[\u001b[33m\"\u001b[39m\u001b[33mweight\u001b[39m\u001b[33m\"\u001b[39m] = unbound_weights\n\u001b[32m      8\u001b[39m all_tickers_df = valid_tickers_df.copy().sort_values(\u001b[33m'\u001b[39m\u001b[33mweight\u001b[39m\u001b[33m'\u001b[39m, ascending=\u001b[38;5;28;01mFalse\u001b[39;00m).reset_index(drop=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 21\u001b[39m, in \u001b[36munbounded_optimization\u001b[39m\u001b[34m(cov_matrix)\u001b[39m\n\u001b[32m     18\u001b[39m weight_bounds = [(\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m)] * num_assets \u001b[38;5;66;03m# Does not allow short selling\u001b[39;00m\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# Finds the resilt of the minimization of the port_variance function, using the initial guess, keeping the cov_matrix constant using the SLSQP method, and with the above listed constraint\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m result = \u001b[43mminimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m=\u001b[49m\u001b[43mport_variance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m=\u001b[49m\u001b[43minitial_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcov_matrix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mSLSQP\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbounds\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweight_bounds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconstraints\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconstraint\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result.fun, result.x\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yello\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\scipy\\optimize\\_minimize.py:705\u001b[39m, in \u001b[36mminimize\u001b[39m\u001b[34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[39m\n\u001b[32m    702\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m bounds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    703\u001b[39m     \u001b[38;5;66;03m# convert to new-style bounds so we only have to consider one case\u001b[39;00m\n\u001b[32m    704\u001b[39m     bounds = standardize_bounds(bounds, x0, \u001b[33m'\u001b[39m\u001b[33mnew\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m705\u001b[39m     bounds = \u001b[43m_validate_bounds\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbounds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmeth\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    707\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m meth \u001b[38;5;129;01min\u001b[39;00m {\u001b[33m\"\u001b[39m\u001b[33mtnc\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mslsqp\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33ml-bfgs-b\u001b[39m\u001b[33m\"\u001b[39m}:\n\u001b[32m    708\u001b[39m         \u001b[38;5;66;03m# These methods can't take the finite-difference derivatives they\u001b[39;00m\n\u001b[32m    709\u001b[39m         \u001b[38;5;66;03m# need when a variable is fixed by the bounds. To avoid this issue,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    714\u001b[39m \n\u001b[32m    715\u001b[39m         \u001b[38;5;66;03m# determine whether any variables are fixed\u001b[39;00m\n\u001b[32m    716\u001b[39m         i_fixed = (bounds.lb == bounds.ub)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yello\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\scipy\\optimize\\_minimize.py:1117\u001b[39m, in \u001b[36m_validate_bounds\u001b[39m\u001b[34m(bounds, x0, meth)\u001b[39m\n\u001b[32m   1115\u001b[39m     bounds.ub = np.broadcast_to(bounds.ub, x0.shape)\n\u001b[32m   1116\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m-> \u001b[39m\u001b[32m1117\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m   1119\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m bounds\n",
      "\u001b[31mValueError\u001b[39m: The number of bounds is not compatible with the length of `x0`."
     ]
    }
   ],
   "source": [
    "# Execute filtering after reading in csv. \n",
    "valid_tickers_df = format_tickers(csv_file_path) \n",
    "\n",
    "covariance_matrix = ((get_calculations(valid_tickers_df[\"Ticker\"].tolist(), returns_start, returns_end))['Covariance']).to_numpy()\n",
    "unbound_var, unbound_weights = unbounded_optimization(covariance_matrix)\n",
    "\n",
    "valid_tickers_df[\"weight\"] = unbound_weights\n",
    "all_tickers_df = valid_tickers_df.copy().sort_values('weight', ascending=False).reset_index(drop=True)\n",
    "\n",
    "all_tickers_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_values = find_optimized_portfolio(all_tickers_df)\n",
    "print(\"================================================================================================================================================\")\n",
    "print(f\"The smallest variance found is {target_values[0]} which is determined from the following portfolio:'\\n{target_values[1]},\\nat the following weights:\\n{target_values[2]}.\")\n",
    "print(\"================================================================================================================================================\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
