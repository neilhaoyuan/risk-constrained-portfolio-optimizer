{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "#These are the libraries you can use.  You may add any libraries directy related to threading if this is a direction\n",
    "#you wish to go (this is not from the course, so it's entirely on you if you wish to use threading).  Any\n",
    "#further libraries you wish to use you must email me, james@uwaterloo.ca, for permission.\n",
    "\n",
    "from IPython.display import display, Math, Latex\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numpy_financial as npf\n",
    "import yfinance as yf\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from datetime import datetime\n",
    "import scipy as sp \n",
    "from scipy.optimize import minimize\n",
    "import itertools as itr\n",
    "from threading import Thread"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group Assignment\n",
    "### Team Number: 15\n",
    "### Team Member Names: Neil Zhang, Rahim Rehan, Krish Patel\n",
    "### Team Strategy Chosen: Risk-Free "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input Filtering\n",
    "The following code will read in `Tickers.csv` and then perform filtration. The primary goal is to remove any duplicate tickers, any non US or Canadian companies, and any stocks that don’t trade at least 5000 shares between Oct 1, 2024 and Sep 30, 2025. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary Filtering\n",
    "Using the `valid_tickers` function we can do a preliminary filtration by:\n",
    "\n",
    "- Using `pandas` to read in the csv and then drop any duplicates tickers\n",
    "- Downloading all the ticker information using `yfinance.download`. We will be using the period of Oct 1, 2024 to Oct 1, 2024 as finance excludes the last day\n",
    "- Manipulating the dataframe to remove any months that trade less than 18 days, and removing stocks that don’t meet our volume requirement "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_tickers(csv_file_path):\n",
    "   \"\"\"\n",
    "   valid_tickers is the prelimary checker, it reads a csv and then removes any tickers that trade less than 5000 shares a month\n",
    "\n",
    "   :param csv_file_path: A string that represents the csv file name that will be read\n",
    "   :return: A list of names of the valid tickers\n",
    "   \"\"\"\n",
    "   tickers_df = pd.read_csv(csv_file_path, header=None)\n",
    "   tickers_df.drop_duplicates(inplace=True)\n",
    "   tickers_df.columns = [\"Ticker\"]\n",
    "   ticker_list = tickers_df[\"Ticker\"].tolist()\n",
    "\n",
    "   # Bulk download the ticker information\n",
    "   hist = yf.download(\n",
    "       tickers=ticker_list,\n",
    "       start=\"2024-10-01\",\n",
    "       end=\"2025-10-01\",\n",
    "       group_by=\"ticker\",\n",
    "       auto_adjust=False,\n",
    "       threads=True\n",
    "    )\n",
    "   \n",
    "   # Count the rows per month per ticker\n",
    "   counts = hist.groupby(hist.index.to_period(\"M\")).transform(\"count\")\n",
    "\n",
    "   # Keep only rows where all tickers in that month have >= 18 trading days\n",
    "   valid_days = counts >= 18\n",
    "\n",
    "   # Remove the months that do not meet threshold and any tickers that don't have a large enough trading volume\n",
    "   hist_filtered = hist[valid_days]\n",
    "   vol_filtered = hist_filtered.xs(\"Volume\", axis=1, level=1)\n",
    "   avg_vol_filtered = vol_filtered.mean()\n",
    "   valid_tickers = avg_vol_filtered[avg_vol_filtered >= 5000].index.tolist()\n",
    "\n",
    "   return valid_tickers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Secondary Filtering: Threaded\n",
    "Now that we have a list of tickers that have passed our preliminary filtering, we want to remove any non US or Canadian companies. \n",
    "\n",
    "But since we need to use yfinance's `info` function, this process will be incredibly slow. Thus to reduce runtime we have decided to thread the secondary filtering, which is:\n",
    "\n",
    "- Using `info` to get the `Currency` of a stock and if it isn’t traded in `CAD` or `USD` then return `NaN`\n",
    "- We keep track of each valid ticker’s `Currency`, `Sector`, and `MarketCap`, all information that will be useful later\n",
    "\n",
    "In the end we turn this information into a final `Dataframe` which is returned. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Begin threading set up\n",
    "def get_format_info(ticker, ticker_data):\n",
    "   \"\"\"\n",
    "   get_format_info is the threading worker function, it is what will be done in parallel. It finds wanted ticker info\n",
    "   and adds it to a list\n",
    "\n",
    "   :param ticker: The ticker whose info is wanted\n",
    "   :param ticker_data: A list that keeps track of the ticker info thats been extracted\n",
    "   \"\"\"\n",
    "   ticker_info = yf.Ticker(ticker).info\n",
    "\n",
    "   # Get the currency, recall we only want USD and CAD traded stocks\n",
    "   currency = ticker_info.get(\"currency\")\n",
    "   if (currency not in [\"USD\", \"CAD\"]):\n",
    "      currency = \"NaN\"\n",
    "\n",
    "   ticker_data.append((ticker, ticker_info.get(\"sector\"), currency, ticker_info.get(\"marketCap\")))\n",
    "\n",
    "#Threading occurs\n",
    "def format_tickers(csv_file_path):\n",
    "   \"\"\"\n",
    "   format_tickers is the main function that performs threading onto the worker function and gets the dataframe of tickers and wanted info\n",
    "\n",
    "   :param csv_file_path: A string that represents the csv file name that will be read\n",
    "   :return: A Dataframe that contains all the info we want for every valid ticker\n",
    "   \"\"\"\n",
    "   ticker_list = valid_tickers(csv_file_path)\n",
    "   \n",
    "   # Threading set up, ticker_data is the shared list where threads store their results, threads is the list of the thread objects\n",
    "   ticker_data = []\n",
    "   threads = []\n",
    "\n",
    "   # Loop and create threads of each ticker in the ticker_list\n",
    "   for t in ticker_list:\n",
    "      thread = Thread(target=get_format_info, args=(t, ticker_data))\n",
    "      threads.append(thread)\n",
    "      thread.start()\n",
    "\n",
    "   # Once all threads complete, join them together \n",
    "   for th in threads:\n",
    "      th.join()\n",
    "\n",
    "   # Create and clean up the Dataframe\n",
    "   ticker_df = pd.DataFrame(ticker_data, columns=[\"Ticker\",\"Sector\",\"Currency\",\"MarketCap\"]) \n",
    "   ticker_df = ticker_df[ticker_df[\"Currency\"] != \"NaN\"]\n",
    "   ticker_df = ticker_df.reset_index(drop=True)\n",
    "   return ticker_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Info Extraction\n",
    "Now armed with the valid tickers, info extraction can be done. The most important thing to consider are the price movements of each stock, but since the period that will be considered is quite long, keeping track of daily data is both memory-intensive and not properly representative. What’s more important is how stocks behave **week by week**. \n",
    "\n",
    "The code below will find the weekly prices and weekly price percent changes of each stock. Using that information the following information can be obtained: `Variance`, `Standard Deviation`,  `Covariance Matrix`, and `Correlation Matrix`,\n",
    "\n",
    "Recall that our strategy is to obtain a ***Risk-Free Portfolio***, which means we want a portfolio whose value does not move. In other words, absolutely no volatility, absolutely no concentration, absolutely no risk, and complete diversification. Those 4 metrics will help it comes to crafting such a portfolio as:\n",
    "\n",
    "- `Variance` tells us how volatile a stock’s returns are from its mean return\n",
    "- `Standard Deviation` also tells us how volatile a stock is\n",
    "- A `Covariance Matrix` tells us how the variance between two assets correlate, giving us information on how to match different stocks together to minimize volatility\n",
    "- A `Correlation Matrix` shows the strength of the relationship between two assets movements "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  143 of 143 completed\n",
      "\n",
      "10 Failed downloads:\n",
      "['DFS', 'ATVI', 'GIB.A.TO', 'AGN', 'BRK.B', 'CELG', 'PTR', 'MON', 'ZZZ.TO', 'RTN']: YFTzMissingError('possibly delisted; no timezone found')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n# Access each piece like:\\ndisplay(primary_calculations['Covariance'])\\ndisplay(primary_calculations['Std_Dev'])\\n\""
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "returns_start = \"2024-11-14\"\n",
    "returns_end = \"2025-11-14\"\n",
    "\n",
    "# Function to return a list of all tickers (first column elements)\n",
    "def get_ticker_list (tickers_df):\n",
    "     \"\"\"\n",
    "     get_ticker_list returns a Python list of tickers\n",
    "\n",
    "     :param tickers_df: A Dataframe of tickers\n",
    "     :return: A list of tickers\n",
    "     \"\"\"\n",
    "     return tickers_df.iloc[:, 0].tolist()\n",
    "\n",
    "# Gets weekly closes of all the stocks in a list of tickers\n",
    "def get_weekly_closes (ticker_lst, start_date, end_date):\n",
    "    \"\"\"     \n",
    "    get_weekly_closes performs data extraction to get weekly closing price info of stocks\n",
    "\n",
    "    :param ticker_lst: List that holds all tickers\n",
    "    :param start_date: start date for calculations\n",
    "    :param end_date: end date for calculations\n",
    "    :return: returns a Dataframe that holds the weekly closing price of each ticker on every week\n",
    "    \"\"\"\n",
    "    #Define a dataframe to hold weekly close prices (checks every friday)\n",
    "    cols = [] # list of Series to concat\n",
    "    #Extract the weekly close prices and store them in the dataframe\n",
    "    for i in ticker_lst:\n",
    "        ticker = yf.Ticker(i)\n",
    "        data = ticker.history(start=start_date, end=end_date)\n",
    "        data.index = pd.to_datetime(data.index) # ensure datetime index\n",
    "        #last() takes the last trading price of the week\n",
    "        series = data['Close'].resample('W-FRI').last()\n",
    "        series.name = f\"Close {i}\" # name each column\n",
    "        cols.append(series) # store weekly closes for each ticker\n",
    "        \n",
    "    # Concatenate all ticker series at once to avoid fragmentation\n",
    "    weekly_closes = pd.concat(cols, axis=1)\n",
    "    # Strip time\n",
    "    weekly_closes.index = weekly_closes.index.strftime('%Y-%m-%d')\n",
    "    return weekly_closes\n",
    "\n",
    "# Creates a df with the (weekly) %change for each column\n",
    "def get_percent_change (closes, start_date, end_date):\n",
    "    \"\"\"     \n",
    "    get_percent_change performs calculations that determines the percent change of closing prices\n",
    "\n",
    "    :param closes: Dataframe that holds closing prices\n",
    "    :param start_date: start date for calculations\n",
    "    :param end_date: end date for calculations\n",
    "    :return: returns a Dataframe that holds percent changes of closing prices\n",
    "    \"\"\"\n",
    "    cols = [] # list of Series to concat\n",
    "    \n",
    "    for i in closes:\n",
    "        col_name = i[6:] # name each column by the ticker from \"Close ---\"\n",
    "        # calculate %change\n",
    "        series = closes[i].pct_change(fill_method=None) # Fill_method=None to hande delisted stocks\n",
    "        series.name = f\"% Change {col_name}\" # name each column\n",
    "        cols.append(series) # store %change for each ticker\n",
    "\n",
    "    # Concatenate all ticker series at once to avoid fragmentation\n",
    "    percent_change = pd.concat(cols, axis=1)\n",
    "    return percent_change\n",
    "\n",
    "# Calculate covariance, correlation, variance, standard deviation\n",
    "def get_calculations(ticker_list, start_date, end_date):\n",
    "    \"\"\"     \n",
    "    get_calculations performs calculations for covariance, correlation, variance, standard deviation\n",
    "\n",
    "    :param ticker_list: list of all tickers\n",
    "    :param start_date: start date for calculations\n",
    "    :param end_date: end date for calculations\n",
    "    :return: returns a dictionary that holds covariance, correlation, standard deviation, and variance \n",
    "    \"\"\"\n",
    "    weekly_closes = get_weekly_closes(ticker_list, start_date, end_date)\n",
    "    weekly_percent_change = get_percent_change(weekly_closes, start_date, end_date)\n",
    "    covariance_matrix = {\n",
    "        'Covariance': weekly_percent_change.cov(),\n",
    "        'Correlation': weekly_percent_change.corr(),\n",
    "        'Variance': weekly_percent_change.var(),\n",
    "        'Std_Dev': weekly_percent_change.std()}\n",
    "    return covariance_matrix\n",
    "\n",
    "info_df = format_tickers(\"Extended_Tickers_Example.csv\") #NOTE: CHANGE FILE NAME BEFORE SUBMITTING\n",
    "ticker_list = (get_ticker_list(info_df)) # List of all tickers\n",
    "primary_calculations = get_calculations(ticker_list, returns_start, returns_end) # Covariance matrix\n",
    "\"\"\"\n",
    "# Access each piece like:\n",
    "display(primary_calculations['Covariance'])\n",
    "display(primary_calculations['Std_Dev'])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Portfolio Variance\n",
    "Our chosen strategy is to create a ***Risk-Free Portfolio***, but what does that mean? In academic terms a risk free asset has:\n",
    "$$\n",
    "\\sigma^2  = 0 \n",
    "$$\n",
    "Meaning its return is constant and guaranteed, with no random variation. Thus a ***Risk-Free Portfolio*** needs a variance of 0. \n",
    "\n",
    "While finding the variation of a singular asset is simple, a portfolio's variance is much harder. After all, different assets clash and diversification will play a part in reducing variance. \n",
    "\n",
    "Now we could implement a method that finds the percent returns of the entire portfolio and then calculates its variance, but such a method is lengthy and difficult to mathematically optimize. Thankfully, there exists a formula that calculates a portfolio’s variance:\n",
    "\n",
    "$$\n",
    "\\sigma_p^2 = \\sum_{i=1}^{n} \\sum_{j=1}^{n} w_i w_j \\sigma_{ij}\n",
    "$$\n",
    "\n",
    "Where w_i, w_j are the weights of assets and \\sigma_{ij} is the covariance. This formula can also be rewritten for matrix calculations (which we want):\n",
    "\n",
    "$$\n",
    "\\sigma_p^2 = \\quad w^T \\Sigma w\n",
    "$$\n",
    "\n",
    "Where w represents the weight of each asset and $\\Sigma$ is a 2D matrix that holds the covariance between each asset. \n",
    "\n",
    "By using that variance formula, we can determine the variance of a portfolio without needing to find the value of the portfolio first. The following code is a function that implements this formula. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the function we want to minimize, aka the minimum variance function\n",
    "def port_variance(weights, cov_matrix):\n",
    "    \"\"\"     \n",
    "    port_variance is the function that calculates the variance of a portfolio. It performs \n",
    "    dot product/matrix multiplication on the weights and covariance matrixes. \n",
    "\n",
    "    :param weights: an array that represents the weight of each asset\n",
    "    :param cov_matrix: a 2D matrix that represents the covariance between each asset \n",
    "    :return: variance of the portfolio\n",
    "    \"\"\"\n",
    "    weights_col = weights.reshape(-1, 1) # Turns into column vector\n",
    "    port_var = np.dot(weights_col.transpose(), (np.dot(cov_matrix, weights_col))) # Doing dot product \n",
    "    return port_var[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding Risk-Free: How Do We Minimum Variance?\n",
    "Getting a variance of 0 is realistically not possible, but what is possible is minimizing variance. Our goal is to create a portfolio with the smallest possible variance. \n",
    "\n",
    "The most obvious way we can do that is find the lowest variance after constructing every single possible portfolio of 10-25 assets from our `N` tickers. But such a task would require too much computation. \n",
    "- With only 50 stocks, there would be over `1.027x10^10` ways to form a portfolio of just 10 assets \n",
    "We need a smarter method. \n",
    "\n",
    "A smarter method is using a ranking approach. \n",
    "- Every valid stock is ranked on how much they can help with minimizing a portfolio’s variance\n",
    "- Then we can create `15` portfolios that hold the top 10, 11, 12…, 25 etc, most impactful stocks \n",
    "- Then find the lowest variance portfolio of those 15 portfolios\n",
    "\n",
    "This method is significantly more efficient, as we no longer have to compute the variance of `>1.027x10^10` portfolios and instead just `15`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ranking Approach\n",
    "\n",
    "But there lies an issue. \n",
    "- How do we rank the impactfulness of a stock on minimizing variance? \n",
    "- Base it on a stock’s `Variance`? But then what if it has unwanted correlations? \n",
    "- What if we choose `Covariance`, or `Correlation`, or maybe we all 3. But then how do we determine which metric is more important than the others? \n",
    "\n",
    "So rather than trying to base a stock’s impactfulness on a metric, let's go straight to what we really want: minimum variance and its correlated `weightings`. After all, higher weighting imply more impactfulness. \n",
    "\n",
    "### Primary Optimization\n",
    "We can rank a stock’s impactfulness on minimizing variance by turning our port_variance function into an optimization problem using `SciPy`. \n",
    "\n",
    "If we give port_variance the covariance matrix of all `N` valid tickers and not their weightings, it will return the minimum variance and the optimized weightings. \n",
    "\n",
    "Now by sorting through these weightings from highest to lowest, we have now ranked the impactfulness. The higher the weighting of an asset, the more important it was to minimizing variance.\n",
    "\n",
    "The following function performs this primary optimization (though it doesn’t do the sort just yet)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primary minimization, there is bounds in this \n",
    "def primary_minimization(cov_matrix):\n",
    "    \"\"\"     \n",
    "    primary_minimization is the function that finds the weightings that result in the mimimum variance. \n",
    "    It performs this using scipy optimization. This perimary version does not consider bounds. \n",
    "\n",
    "    :param cov_matrix: a 2D matrix that represents the covariance between each asset \n",
    "    :return: returns the minimum variance and the weightings associated with that\n",
    "    \"\"\"\n",
    "    num_assets = cov_matrix.shape[0]\n",
    "    initial_weight = [1/num_assets] * num_assets # The initial guess of the weights\n",
    "\n",
    "    constraint = {\n",
    "        'type':'eq', # Constraint type is equality\n",
    "        'fun': lambda w: sum(w) - 1 # The function's weight's must sum to 1\n",
    "        }\n",
    "    \n",
    "    weight_bounds = [(0, 1)] * num_assets # Does not allow short selling\n",
    "    \n",
    "    # Finds the resilt of the minimization of the port_variance function, using the initial guess, keeping the cov_matrix constant using the SLSQP method, and with the above listed constraint\n",
    "    result = minimize(fun=port_variance, x0=initial_weight, args=(cov_matrix,), method='SLSQP', bounds=weight_bounds, constraints=constraint)\n",
    "    return result.fun, result.x\n",
    "\n",
    "pd_cov_matrix = primary_calculations['Covariance']\n",
    "numpy_cov_matrix = pd_cov_matrix.to_numpy() # Matrix of covariances of assets\n",
    "primary_var, primary_weights = primary_minimization(numpy_cov_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Secondary Optimization\n",
    "Now with the Primary Optimization, we can create a Secondary Optimization model that properly optimizes a portfolio to respect weighting bounds. With `SciPy` we can establish `min_weight` and `max_weight` of assets. Now no asset can be more than 15% and none below (100/2n)% where `n` represents the number of stocks. \n",
    "\n",
    "The following function performs this secondary optimization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The secondary optimization that includes the bounds \n",
    "def secondary_minimization(cov_matrix):\n",
    "    \"\"\"     \n",
    "    secondary_minimization is the function that finds the weightings that result in the mimimum variance while considering the bounds.\n",
    "    That is, it ensures the weightings \n",
    "\n",
    "    :param cov_matrix: a 2D matrix that represents the covariance between each asset \n",
    "    :return: returns the minimum variance and the weightings associated with that\n",
    "    \"\"\"\n",
    "    num_assets = len(cov_matrix[0]) \n",
    "    initial_weight = [1/num_assets] * num_assets # The initial guess of the weights\n",
    "\n",
    "    min_weight = (100/(2*num_assets))/100 # Do not need to include portfolio value, because 1 is the portfolio value (and sum of weights)\n",
    "    max_weight = 0.15 # Same as above\n",
    "\n",
    "    constraint = {\n",
    "        'type':'eq', # Constraint type is equality\n",
    "        'fun': lambda w: sum(w) - 1 # The function's weight's must sum to 1\n",
    "        }\n",
    "    \n",
    "    weight_bounds = [(min_weight, max_weight)] * num_assets\n",
    "    \n",
    "    # Finds the resilt of the minimization of the port_variance function, using the initial guess, keeping the cov_matrix constant using the SLSQP method, and with the above listed constraint\n",
    "    result = minimize(fun=port_variance, x0=initial_weight, args=(cov_matrix,), method='SLSQP', bounds = weight_bounds, constraints=constraint)\n",
    "    return result.fun, result.x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ranking Approach: Creating Portfolios\n",
    "With all `N` tickers properly ranked from most to least important, we can start creating the 15 portfolios that hold the 10 to 25 most impactful stocks. \n",
    "\n",
    "However building our 15 portfolios. Each portfolio must contain a `large cap` and a `small cap` company. \n",
    "\n",
    "So when we’re building our portfolio we’re constantly checking if these conditions will be fulfilled. If they’re not then the function tries its best to fulfill them. \n",
    "\n",
    "For example when building a portfolio of size 15, if there is not a `small cap` company in the top 15 but the 16th most important stock is `small cap` then it will build a portfolio with the Rank 16 stock rather than the Rank 15. \n",
    "\n",
    "To put it simply, the `Least Impactful` stock(s) will always be replaced if needed in order to make the portfolio valid. Even if that goes against the Ranking order. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code that creates a portfolio that matches the requirements\n",
    "\n",
    "# Determines all the indexes in a portfolio that are large cap stocks\n",
    "def is_lg_cap(portfolio):\n",
    "    \"\"\"\n",
    "     is_lg_cap determines all the indexes in a portfolio that are large cap stocks\n",
    "\n",
    "     :param portfolio: A list of Series objects that contain a stock's info \n",
    "     :return: A list of indexes of ticker's that are large cap\n",
    "     \"\"\"\n",
    "    lg_cap = []\n",
    "    for i in range(len(portfolio)):\n",
    "        if portfolio[i]['MarketCap'] > 10_000_000_000:\n",
    "            lg_cap.append(i)\n",
    "    return lg_cap\n",
    "\n",
    "# Determines all the indexes in a portfolio that are small cap stocks\n",
    "def is_sm_cap(portfolio):\n",
    "    \"\"\"\n",
    "     is_sm_cap determines all the indexes in a portfolio that are small cap stocks\n",
    "\n",
    "     :param portfolio: A list of Series objects that contain a stock's info \n",
    "     :return: A list of indexes of ticker's that are small cap\n",
    "     \"\"\"\n",
    "    sm_cap = []\n",
    "    for i in range(len(portfolio)):\n",
    "        if portfolio[i]['MarketCap'] < 2_000_000_000:\n",
    "            sm_cap.append(i)\n",
    "    return sm_cap\n",
    "\n",
    "# Determines if an individual stock is a large market cap\n",
    "def is_lg(row):\n",
    "    return row['MarketCap'] > 10_000_000_000\n",
    "\n",
    "# Determines if an individual stock is a small market cap\n",
    "def is_sm(row):\n",
    "    return row['MarketCap'] < 2_000_000_000\n",
    "\n",
    "# Determines the index of the least important stock. Least important in this case is the one with the lowest weighting in the ordered list, but\n",
    "# if the least important is either the ONLY SMALL or LARGE cap stock then the second least important stock is now designated the least important\n",
    "def find_least_imp(portfolio, lg_indxs, sm_indxs):\n",
    "    \"\"\" \n",
    "    find_least_imp Determines the index of the least important stock. Least important in this case is the one with the lowest weighting in the ordered list, but \n",
    "    if the least important is either the ONLY SMALL or LARGE cap stock then the second least important stock is now designated the least important\n",
    "\n",
    "    :param portfolio: A list of stock data in Series format\n",
    "    :param lg_indxs: A list of all indexes that hold large cap stocks\n",
    "    :param sm_indxs: A list of all indexes that hold small cap stocks\n",
    "    :return: integer that represents the index that is desginated least important\n",
    "    \"\"\"\n",
    "\n",
    "    only_large_idx = None\n",
    "    only_small_idx = None\n",
    "\n",
    "    # Determines the protected indexes\n",
    "    if len(lg_indxs) == 1:\n",
    "        only_large_idx = lg_indxs[0]\n",
    "    if len(sm_indxs) == 1:\n",
    "        only_small_idx = sm_indxs[0]\n",
    "\n",
    "    for i in range(len(portfolio) - 1, -1, -1):\n",
    "        if i != only_large_idx and i != only_small_idx:\n",
    "            return i\n",
    "\n",
    "def valid_port_check(ticker_list):\n",
    "    \"\"\"\n",
    "    valid_port_check checks if a portfolio fits all sector and weighting constraints\n",
    "\n",
    "    :param ticker_list: A list of ticker names\n",
    "    :return: Boolean value depending on if its true or not\n",
    "    \"\"\"\n",
    "    portfolio = [] # Will be a list of Series\n",
    "    port_sectors = []\n",
    "    max_sector_num = int(len(ticker_list) * 0.4)\n",
    "\n",
    "    # Creates preliminary portfolio \n",
    "    for i in range(len(ticker_list)):\n",
    "        cur = ordered_info_df[ordered_info_df[\"Ticker\"] == ticker_list[i]].iloc[0]\n",
    "        cur_sector = cur['Sector']\n",
    "        portfolio.append(cur)\n",
    "        port_sectors.append(cur_sector)\n",
    "    \n",
    "    for s in port_sectors:\n",
    "        if ((port_sectors.count(s)) > max_sector_num):\n",
    "            return False\n",
    "\n",
    "    num_lg = len(is_lg_cap(portfolio))\n",
    "    num_sm = len(is_sm_cap(portfolio))\n",
    "    if (num_lg == 0 or num_sm == 0):  \n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "\n",
    "# Creates a portfolio that is valid, aside from the 40% sector restriction\n",
    "def create_portfolio(size):\n",
    "    \"\"\" \n",
    "    create_portfolio creates a portfolio containing both small and large cap\n",
    "\n",
    "    :param size: the size of the portfolio\n",
    "    :return: a list of tickers that are in the portfolio, or none if it cannot create a portfolio that satisfies all requirements\n",
    "    \"\"\"\n",
    "\n",
    "    portfolio = [] # Will be a list of Series\n",
    "    i = 0\n",
    "    ticker_only = []\n",
    "    \n",
    "    # Creates preliminary portfolio \n",
    "    while len(portfolio) < size:\n",
    "        if i >= len(ordered_info_df):\n",
    "            return None\n",
    "        cur = ordered_info_df.iloc[i]\n",
    "        portfolio.append(cur)\n",
    "        i += 1\n",
    "\n",
    "    lg_idxs = is_lg_cap(portfolio)\n",
    "    sm_idxs = is_sm_cap(portfolio)\n",
    "\n",
    "    # Fixes the no large market cap issue\n",
    "    while len(lg_idxs) == 0: \n",
    "        replaced = find_least_imp(portfolio, lg_idxs, sm_idxs)\n",
    "        if i >= len(ordered_info_df):\n",
    "            return None\n",
    "        cur = ordered_info_df.iloc[i]\n",
    "        \n",
    "        if (is_lg(cur)):\n",
    "            portfolio[replaced] = cur\n",
    "\n",
    "        lg_idxs = is_lg_cap(portfolio)\n",
    "        sm_idxs = is_sm_cap(portfolio)\n",
    "        i += 1\n",
    "\n",
    "    # Fixes the no small market cap issues\n",
    "    while len(sm_idxs) == 0: \n",
    "        replaced = find_least_imp(portfolio, lg_idxs, sm_idxs)\n",
    "        if i >= len(ordered_info_df):\n",
    "            return None\n",
    "        cur = ordered_info_df.iloc[i]\n",
    "        \n",
    "        if (is_sm(cur)):\n",
    "            portfolio[replaced] = cur\n",
    "        lg_idxs = is_lg_cap(portfolio)\n",
    "        sm_idxs = is_sm_cap(portfolio)\n",
    "        i += 1\n",
    "    \n",
    "    for stock in portfolio:\n",
    "        ticker_name = stock[\"Ticker\"]\n",
    "        ticker_only.append(ticker_name)\n",
    "    \n",
    "    return ticker_only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sector Limits\n",
    "\n",
    "One extremely important restriction in our portfolio is that no sector can be worth more than 40% of the portfolio's value. However there is a problem, to see if a sector is worth more than 40% we must first know the weights of each stock. That means we have to run the secondary optimization **before** checking the sector limits. \n",
    "\n",
    "We have to perform a two-stage optimization. \n",
    "\n",
    "After running secondary optimization we check if any sector limits are broken, if they are we go and replace the least important stock in the heaviest sector with a replacement. Then we check again if this replacement has fulfilled the sector limit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_sector_weights(tickers, weights, info_df):\n",
    "    \"\"\"\n",
    "    Calculate how much of the portfolio, by weight, belongs to each sector.\n",
    "    :param tickers: list of ticker names \n",
    "    :param weights: list of weights from the optimizer \n",
    "    :param info_df: dataframe that includes Ticker and Sector columns\n",
    "    :return: the sector weight total\n",
    "    \"\"\"\n",
    "\n",
    "    # Make lookup by ticker\n",
    "    info_by_ticker = info_df.set_index(\"Ticker\")\n",
    "\n",
    "    sector_totals = {}  # Holds sum of weights for each sector\n",
    "\n",
    "    # Loop through\n",
    "    for ticker, wt in zip(tickers, weights):\n",
    "        sector = info_by_ticker.loc[ticker, \"Sector\"]\n",
    "        \n",
    "        # Add weight to appropriate sector\n",
    "        if sector not in sector_totals:\n",
    "            sector_totals[sector] = 0.0\n",
    "        sector_totals[sector] += float(wt)\n",
    "\n",
    "    return sector_totals\n",
    "\n",
    "def optimize_with_sector_limit(initial_tickers, returns_start, returns_end, info_df):\n",
    "    \"\"\"\n",
    "    optimize_with_sector_limit will run optimization and enforce 40% sector-by-value constraint by replacing the\n",
    "    smallest-weight stock in the overweight sector until constraints are satisfied or no replacements are possible\n",
    "    :param initial_tickers: The list of ticker symbols that is the starting portfolio before optimization\n",
    "    :param returns_start: A date\n",
    "    :param returns_end: A date\n",
    "    :param info_df: A DataFrame containing the Ticker and Sector columns, in our case its gonna be ordered_info_df\n",
    "    \"\"\"\n",
    "\n",
    "    current_tickers = list(initial_tickers)\n",
    "    used_tickers = set(current_tickers)\n",
    "\n",
    "    # Maximum range is 20, only go through 20 iterations before stopping to prevent runtime loss\n",
    "    for n in range(20):\n",
    "        # Get values needed for secondary optimization\n",
    "        calc_df = get_calculations(current_tickers, returns_start, returns_end)\n",
    "        cov_np = calc_df[\"Covariance\"].to_numpy()\n",
    "\n",
    "        portfolio_variance, weights = secondary_minimization(cov_np)\n",
    "        weights = np.array(weights).flatten() \n",
    "\n",
    "        # Get each sectors weights\n",
    "        sector_weights = compute_sector_weights(current_tickers, weights, info_df)\n",
    "\n",
    "        # Find the sector with the largest weight, if this one is valid that all others are valid\n",
    "        heaviest_sector = None\n",
    "        heaviest_weight = -1.0\n",
    "        for sector, w in sector_weights.items():\n",
    "            if w > heaviest_weight:\n",
    "                heaviest_sector = sector\n",
    "                heaviest_weight = w\n",
    "\n",
    "        # Check if the heaviest weight is under the max, if so return the information\n",
    "        if heaviest_weight <= 0.4:\n",
    "            return current_tickers, portfolio_variance, weights\n",
    "\n",
    "        # Find the least weighted, aka least important, stock of the heaviest sector\n",
    "        info_indexed = info_df.set_index(\"Ticker\")\n",
    "\n",
    "        smallest_weight = 100000 # Placeholder\n",
    "        smallest_index = None\n",
    "\n",
    "        for i in range(len(current_tickers)):\n",
    "            sector_name = info_indexed.at[current_tickers[i], \"Sector\"]\n",
    "            if sector_name == heaviest_sector:\n",
    "                if weights[i] < smallest_weight:\n",
    "                    smallest_weight = weights[i]\n",
    "                    smallest_index = i\n",
    "\n",
    "        # Get a replacement ticker to replace the least important heaviest sector \n",
    "        replacement_ticker = None\n",
    "        for row in info_df.itertuples(index=False):\n",
    "            candidate = row.Ticker\n",
    "            if candidate not in used_tickers:\n",
    "                replacement_ticker = candidate\n",
    "                break\n",
    "\n",
    "        # If no new ticker available then stop trying to fix portfolio\n",
    "        if replacement_ticker is None:\n",
    "            return None, None, None\n",
    "\n",
    "        # Replace smallest-weight stock in overweight sector\n",
    "        used_tickers.add(replacement_ticker)\n",
    "        current_tickers[smallest_index] = replacement_ticker\n",
    "\n",
    "    # End loop fallback once we go over iterations\n",
    "    return None, None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result\n",
    "\n",
    "The following code puts this all together. Runs the primary optimization and then creates the 15 portfolios, and then determines which one results in the smallest variance using the secondary optimization whilst ensuring it satisfies all restrictions (including sector).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The smallest variance found is 7.889818973904907e-05 which is determined from the following portfolio: ['DG', 'FTS.TO', 'CME', 'DUK', 'AEP', 'EXC', 'ENB.TO', 'WN.TO', 'BNS.TO', 'UL', 'BB.TO', 'BTI', 'LMT', 'KO', 'T.TO', 'SLF.TO', 'BCE.TO', 'COST', 'ATD.TO', 'TRP.TO', 'GOOG', 'WCN.TO', 'RY.TO', 'NA.TO', 'KITS.TO'], at the following weights [0.04 0.04 0.04 0.04 0.04 0.04 0.04 0.04 0.04 0.04 0.04 0.04 0.04 0.04\n",
      " 0.04 0.04 0.04 0.04 0.04 0.04 0.04 0.04 0.04 0.04 0.04].\n"
     ]
    }
   ],
   "source": [
    "info_df['weight'] = primary_weights #Assume info_df holds the tickers, sector, market cap, etc and not the dates etc. We now add the weights.\n",
    "ordered_info_df = info_df.copy().sort_values('weight', ascending = False).reset_index(drop=True)\n",
    "\n",
    "ordered_info_df\n",
    "\n",
    "all_ports = []\n",
    "all_variance = []\n",
    "all_weights = []\n",
    "count = 0\n",
    "while (count + 10) < 26:\n",
    "\n",
    "    size = count + 10\n",
    "    base_port = create_portfolio(size)\n",
    "\n",
    "    if base_port is None:\n",
    "        print(f\"Failed to build portfolio of size {count+10}, skipping.\")\n",
    "        count += 1\n",
    "        continue\n",
    "\n",
    "    final_port, final_var, final_w = optimize_with_sector_limit(base_port, returns_start, returns_end, ordered_info_df)\n",
    "    if final_port is None:\n",
    "        print(f\"Sector constraint failed for portfolio size {count+10}, skipping.\")\n",
    "        count += 1\n",
    "        continue\n",
    "\n",
    "    all_ports.append(final_port)\n",
    "    all_variance.append(final_var)\n",
    "    all_weights.append(final_w)\n",
    "    count += 1\n",
    "\n",
    "if not all_variance:\n",
    "    print(\"No valid portfolios were generated for some reason. Please check ticker csv.\")\n",
    "else:\n",
    "    smallest_var = min(all_variance)\n",
    "    index = all_variance.index(smallest_var)\n",
    "    target = [smallest_var, all_ports[index], all_weights[index]]\n",
    "    print(f\"The smallest variance found is {target[0]} which is determined from the following portfolio: {target[1]}, at the following weights {target[2]}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Purchasing Shares\n",
    "\n",
    "Using the weightings of the minimized variance, it purchases stocks at a specific day's closing price at those weights. \n",
    "\n",
    "When purchasing we consider two fee methods, one where we pay a flat $2.15 USD fee, and another where each share costs $0.001 USD (note we convert all our calculations and prices into CAD). \n",
    "\n",
    "We perform both of these methods, and choose the method that lets us spend the closest to our target budget of $1,000,000 CAD. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_close_prices_and_rate(tickers, target_date, end_date):\n",
    "    \"\"\"\n",
    "    get_close_prices_and_rate finds the closing price of tickers on a date, and the exchange rate on a date\n",
    "\n",
    "    :param tickers: list of tickers\n",
    "    :param target_data: the day of price we want, normally most recent business day\n",
    "    :param end_date: the day after, as yfinance is not inclusive\n",
    "    :return: a Series that contains the target days close price\n",
    "    :return: the USD to CAD exchange rate\n",
    "    \"\"\"\n",
    "    data = yf.download(tickers, start=target_date, end=end_date)[\"Close\"]\n",
    "    close_prices = data.iloc[0]\n",
    "\n",
    "    exchange_rate = yf.download(\"CAD=X\", start=target_date, end=end_date)[\"Close\"]\n",
    "    exchange_rate = exchange_rate.iloc[0]\n",
    "\n",
    "    return close_prices, exchange_rate.item()\n",
    "\n",
    "def purchase_flat_fee(df, budget, exchange_rate):\n",
    "    \"\"\"\n",
    "    purchase_flat_fee transforms a Dataframe to include the amount of shares bought with a flat fee, and the value of that\n",
    "\n",
    "    :param df: The Dataframe that will be transformed\n",
    "    :param budget: An Integer representing the budget\n",
    "    :param exchange_rate: A series of the USD-CAD exchange rate\n",
    "    :return: Transformed Dataframe\n",
    "    \"\"\"\n",
    "\n",
    "    df[\"Shares Bought Flat Fee\"] = (df[\"Weight\"] * (budget - (2.5*exchange_rate))) / df[\"Price\"]\n",
    "    df[\"Flat Fee Worth\"] = df[\"Shares Bought Flat Fee\"] * df[\"Price\"]\n",
    "\n",
    "    return df \n",
    "\n",
    "def purchase_variable_fee(df, budget, exchange_rate):\n",
    "    \"\"\"\n",
    "    purchase_variable_fee transforms a Dataframe to include the amount of shares bought with the share-dependent rate\n",
    "\n",
    "    :param df: The Dataframe that will be transformed\n",
    "    :param budget: An Integer representing the budget\n",
    "    :param exchange_rate: A series of the USD-CAD exchange rate\n",
    "    :return: Transformed Dataframe\n",
    "    \"\"\"\n",
    "    df[\"Shares w/o Fee\"] = (df[\"Weight\"] * budget) / df[\"Price\"]\n",
    "    total_shares = df[\"Shares w/o Fee\"].sum()\n",
    "    \n",
    "    variable_fee_usd = total_shares * 0.001\n",
    "    variable_fee_cad = variable_fee_usd * exchange_rate\n",
    "\n",
    "    adjusted_budget = budget - variable_fee_cad\n",
    "    df[\"Shares Bought Variable Fee\"] = (df[\"Weight\"] * adjusted_budget) / df[\"Price\"]\n",
    "    df[\"Variable Fee Worth\"] = df[\"Shares Bought Variable Fee\"] * df[\"Price\"]\n",
    "\n",
    "    return df\n",
    "\n",
    "def ideal_shares(df):\n",
    "    \"\"\" \n",
    "    ideal_shares determines which fee strategy, flat or variable, is the most optimal and transforms Dataframe accordingly\n",
    "\n",
    "    :param df: The Dataframe that will be transformed\n",
    "    :return: Transformed Dataframe\n",
    "    \"\"\"\n",
    "    sum_flat_fee = df[\"Flat Fee Worth\"].sum()\n",
    "    sum_variable_fee = df[\"Variable Fee Worth\"].sum()\n",
    "\n",
    "    if (sum_flat_fee < sum_variable_fee):\n",
    "        df.drop([\"Shares Bought Flat Fee\", \"Flat Fee Worth\", \"Shares w/o Fee\"], axis=1, inplace=True)\n",
    "        df.rename(columns={\"Shares Bought Variable Fee\":\"Shares\", \"Variable Fee Worth\":\"Value\"}, inplace=True)\n",
    "    else:\n",
    "        df.drop([\"Shares Bought Variable Fee\", \"Variable Fee Worth\", \"Shares w/o Fee\"], axis=1, inplace=True)\n",
    "        df.rename(columns={\"Shares Bought Flat Fee\":\"Shares\", \"Flat Fee Worth\":\"Value\"}, inplace=True)\n",
    "    \n",
    "    return df \n",
    "\n",
    "def add_currency(df_small, df_large):\n",
    "    \"\"\" \n",
    "    add_currency adds the currency type onto the Dataframe\n",
    "\n",
    "    :param df_small: The Dataframe that holds the shares bought\n",
    "    :param_df_large: The Dataframe the holds the Currency's\n",
    "    :return: Transformed Dataframe\n",
    "    \"\"\"\n",
    "\n",
    "    df_with_currency = df_small.merge(df_large[[\"Ticker\", \"Currency\"]], on=\"Ticker\", how=\"left\")\n",
    "    return df_with_currency\n",
    "\n",
    "def convert_closing(df, exchange_rate):\n",
    "    \"\"\"\n",
    "    convert_closing converts the price of any stock thats in USD to CAD\n",
    "\n",
    "    :param df: Dataframe to be transformed\n",
    "    :param exchange_rate: The USD-CAD rate\n",
    "    :return: Dataframe with converted prices\n",
    "    \"\"\"\n",
    "\n",
    "    df.loc[df[\"Currency\"] == \"USD\", \"Price\"] = df[\"Price\"] * exchange_rate\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Final Dataframe Output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yello\\AppData\\Local\\Temp\\ipykernel_23748\\1056367655.py:11: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
      "  data = yf.download(tickers, start=target_date, end=end_date)[\"Close\"]\n",
      "[*********************100%***********************]  25 of 25 completed\n",
      "C:\\Users\\yello\\AppData\\Local\\Temp\\ipykernel_23748\\1056367655.py:14: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
      "  exchange_rate = yf.download(\"CAD=X\", start=target_date, end=end_date)[\"Close\"]\n",
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "temp_df = pd.DataFrame({\n",
    "    \"Ticker\": target[1],\n",
    "    \"Weight\": target[2]\n",
    "})\n",
    "\n",
    "target_date = \"2025-11-18\" #Example\n",
    "end_date = \"2025-11-19\" #Example\n",
    "closing, usd_cad_rate = get_close_prices_and_rate(target[1], target_date, end_date)\n",
    "temp_df[\"Price\"] = closing.to_numpy()\n",
    "temp_df = add_currency(temp_df,ordered_info_df)\n",
    "temp_df_cad = convert_closing(temp_df, usd_cad_rate)\n",
    "temp_df_cad = purchase_flat_fee(temp_df_cad, 1_000_000, usd_cad_rate)\n",
    "temp_df_cad = purchase_variable_fee(temp_df_cad, 1_000_000, usd_cad_rate)\n",
    "Portfolio_Final = ideal_shares(temp_df_cad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output Final CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "Portfolio_Final[['Ticker', 'Shares']].to_csv(\"Stocks_Group_15.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Double Checking Weights, Shares, and Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sum of the weights is 1.0, the sum of the shares is 17500.606131598044, the sum of the values is 999996.4873749018.\n"
     ]
    }
   ],
   "source": [
    "sum_weight = Portfolio_Final[\"Weight\"].sum() # Doesn't return 1 properly because floats can't sum to exact integers\n",
    "sum_shares = Portfolio_Final[\"Shares\"].sum()\n",
    "sum_value = Portfolio_Final[\"Value\"].sum()\n",
    "print(f\"The sum of the weights is {sum_weight}, the sum of the shares is {sum_shares}, the sum of the values is {sum_value}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[151]\u001b[39m\u001b[32m, line 46\u001b[39m\n\u001b[32m     41\u001b[39m             optimal_weights = current_weights\n\u001b[32m     43\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m optimal_port, optimal_var, optimal_weights\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m optimal_portfolio, optimal_variance, optimal_weights = \u001b[43mcreate_optimal_portfolio\u001b[49m\u001b[43m(\u001b[49m\u001b[43mordered_ticker_list\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[151]\u001b[39m\u001b[32m, line 30\u001b[39m, in \u001b[36mcreate_optimal_portfolio\u001b[39m\u001b[34m(ordered_ticker_list)\u001b[39m\n\u001b[32m     27\u001b[39m optimal_weights = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m potential_portfolios:\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m     current_port, current_var, current_weights = \u001b[43mcreate_portfolio_combs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     32\u001b[39m     \u001b[38;5;66;03m# Current portfolio failed the requirements\u001b[39;00m\n\u001b[32m     33\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m current_port \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[151]\u001b[39m\u001b[32m, line 9\u001b[39m, in \u001b[36mcreate_portfolio_combs\u001b[39m\u001b[34m(ticker_list)\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate_portfolio_combs\u001b[39m(ticker_list):\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mvalid_port_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43mticker_list\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m     10\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     11\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[144]\u001b[39m\u001b[32m, line 78\u001b[39m, in \u001b[36mvalid_port_check\u001b[39m\u001b[34m(ticker_list)\u001b[39m\n\u001b[32m     76\u001b[39m \u001b[38;5;66;03m# Creates preliminary portfolio \u001b[39;00m\n\u001b[32m     77\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(ticker_list)):\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m     cur = ordered_info_df[\u001b[43mordered_info_df\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mTicker\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[43mticker_list\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m].iloc[\u001b[32m0\u001b[39m]\n\u001b[32m     79\u001b[39m     cur_sector = cur[\u001b[33m'\u001b[39m\u001b[33mSector\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     80\u001b[39m     portfolio.append(cur)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yello\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\pandas\\core\\ops\\common.py:76\u001b[39m, in \u001b[36m_unpack_zerodim_and_defer.<locals>.new_method\u001b[39m\u001b[34m(self, other)\u001b[39m\n\u001b[32m     72\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n\u001b[32m     74\u001b[39m other = item_from_zerodim(other)\n\u001b[32m---> \u001b[39m\u001b[32m76\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yello\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\pandas\\core\\arraylike.py:40\u001b[39m, in \u001b[36mOpsMixin.__eq__\u001b[39m\u001b[34m(self, other)\u001b[39m\n\u001b[32m     38\u001b[39m \u001b[38;5;129m@unpack_zerodim_and_defer\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m__eq__\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__eq__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other):\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_cmp_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperator\u001b[49m\u001b[43m.\u001b[49m\u001b[43meq\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yello\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\pandas\\core\\series.py:6140\u001b[39m, in \u001b[36mSeries._cmp_method\u001b[39m\u001b[34m(self, other, op)\u001b[39m\n\u001b[32m   6136\u001b[39m rvalues = extract_array(other, extract_numpy=\u001b[38;5;28;01mTrue\u001b[39;00m, extract_range=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m   6138\u001b[39m res_values = ops.comparison_op(lvalues, rvalues, op)\n\u001b[32m-> \u001b[39m\u001b[32m6140\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_construct_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43mres_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mres_name\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yello\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\pandas\\core\\series.py:6252\u001b[39m, in \u001b[36mSeries._construct_result\u001b[39m\u001b[34m(self, result, name)\u001b[39m\n\u001b[32m   6249\u001b[39m \u001b[38;5;66;03m# TODO: result should always be ArrayLike, but this fails for some\u001b[39;00m\n\u001b[32m   6250\u001b[39m \u001b[38;5;66;03m#  JSONArray tests\u001b[39;00m\n\u001b[32m   6251\u001b[39m dtype = \u001b[38;5;28mgetattr\u001b[39m(result, \u001b[33m\"\u001b[39m\u001b[33mdtype\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m6252\u001b[39m out = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_constructor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   6253\u001b[39m out = out.__finalize__(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m   6255\u001b[39m \u001b[38;5;66;03m# Set the result's name after __finalize__ is called because __finalize__\u001b[39;00m\n\u001b[32m   6256\u001b[39m \u001b[38;5;66;03m#  would set it back to self.name\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yello\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\pandas\\core\\series.py:596\u001b[39m, in \u001b[36mSeries.__init__\u001b[39m\u001b[34m(self, data, index, dtype, name, copy, fastpath)\u001b[39m\n\u001b[32m    593\u001b[39m         data = SingleArrayManager.from_array(data, index)\n\u001b[32m    595\u001b[39m NDFrame.\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, data)\n\u001b[32m--> \u001b[39m\u001b[32m596\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m = name\n\u001b[32m    597\u001b[39m \u001b[38;5;28mself\u001b[39m._set_axis(\u001b[32m0\u001b[39m, index)\n\u001b[32m    599\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m original_dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m is_pandas_object \u001b[38;5;129;01mand\u001b[39;00m data_dtype == np.object_:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yello\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\pandas\\core\\generic.py:6342\u001b[39m, in \u001b[36mNDFrame.__setattr__\u001b[39m\u001b[34m(self, name, value)\u001b[39m\n\u001b[32m   6339\u001b[39m \u001b[38;5;66;03m# if this fails, go on to more involved attribute setting\u001b[39;00m\n\u001b[32m   6340\u001b[39m \u001b[38;5;66;03m# (note that this matches __getattr__, above).\u001b[39;00m\n\u001b[32m   6341\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._internal_names_set:\n\u001b[32m-> \u001b[39m\u001b[32m6342\u001b[39m     \u001b[38;5;28;43mobject\u001b[39;49m\u001b[43m.\u001b[49m\u001b[34;43m__setattr__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   6343\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._metadata:\n\u001b[32m   6344\u001b[39m     \u001b[38;5;28mobject\u001b[39m.\u001b[34m__setattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, value)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yello\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\pandas\\core\\series.py:789\u001b[39m, in \u001b[36mSeries.name\u001b[39m\u001b[34m(self, value)\u001b[39m\n\u001b[32m    787\u001b[39m \u001b[38;5;129m@name\u001b[39m.setter\n\u001b[32m    788\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mname\u001b[39m(\u001b[38;5;28mself\u001b[39m, value: Hashable) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m789\u001b[39m     \u001b[43mvalidate_all_hashable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__name__\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m.name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    790\u001b[39m     \u001b[38;5;28mobject\u001b[39m.\u001b[34m__setattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m_name\u001b[39m\u001b[33m\"\u001b[39m, value)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yello\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\pandas\\core\\dtypes\\common.py:1581\u001b[39m, in \u001b[36mvalidate_all_hashable\u001b[39m\u001b[34m(error_name, *args)\u001b[39m\n\u001b[32m   1574\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m typ \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m\"\u001b[39m\u001b[33mgeneric\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mns\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m   1575\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1576\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mrepr\u001b[39m(dtype.name)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m is too specific of a frequency, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1577\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mtry passing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mrepr\u001b[39m(dtype.type.\u001b[34m__name__\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1578\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1581\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mvalidate_all_hashable\u001b[39m(*args, error_name: \u001b[38;5;28mstr\u001b[39m | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1582\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1583\u001b[39m \u001b[33;03m    Return None if all args are hashable, else raise a TypeError.\u001b[39;00m\n\u001b[32m   1584\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   1598\u001b[39m \u001b[33;03m    None\u001b[39;00m\n\u001b[32m   1599\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m   1600\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mall\u001b[39m(is_hashable(arg) \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m args):\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# --- Third Optimization --- #\n",
    "\n",
    "# Make a list of the top 30 most important stocks\n",
    "ordered_ticker_list = get_ticker_list(ordered_info_df)\n",
    "ordered_ticker_list = ordered_ticker_list[0:30]\n",
    "\n",
    "\n",
    "def create_portfolio_combs(ticker_list):\n",
    "    if not valid_port_check(ticker_list):\n",
    "        return None, None, None\n",
    "    else:\n",
    "        temp_cov2 = get_calculations(ticker_list, returns_start, returns_end)\n",
    "        cov_np2 = temp_cov2['Covariance'].to_numpy()\n",
    "        temp_var2, temp_weights2 = secondary_minimization(cov_np2)\n",
    "\n",
    "        # Returns the inputted list, the variance, and a list of the weights\n",
    "        return ticker_list, temp_var2, temp_weights2\n",
    "\n",
    "\n",
    "def create_optimal_portfolio(ordered_ticker_list):\n",
    "    # Create list of every possible portfolio\n",
    "    potential_portfolios = list(itr.combinations(ordered_ticker_list, 25))\n",
    "\n",
    "    # Initialize the optimal portfolio data\n",
    "    optimal_port = None\n",
    "    optimal_var = float(\"inf\")       # Infinitely high float\n",
    "    optimal_weights = None\n",
    "\n",
    "    for i in potential_portfolios:\n",
    "        current_port, current_var, current_weights = create_portfolio_combs(i)\n",
    "\n",
    "        # Current portfolio failed the requirements\n",
    "        if current_port is None:\n",
    "            continue # skip\n",
    "        \n",
    "        # If the current portfolio has a lower variance than any previous ones\n",
    "        if current_var <= optimal_var:\n",
    "            # Set the optimal portfolio data to the current\n",
    "            optimal_port = current_port\n",
    "            optimal_var = current_var\n",
    "            optimal_weights = current_weights\n",
    "\n",
    "    return optimal_port, optimal_var, optimal_weights\n",
    "\n",
    "\n",
    "optimal_portfolio, optimal_variance, optimal_weights = create_optimal_portfolio(ordered_ticker_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contribution Declaration\n",
    "\n",
    "The following team members made a meaningful contribution to this assignment:\n",
    "\n",
    "Insert Names Here. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
