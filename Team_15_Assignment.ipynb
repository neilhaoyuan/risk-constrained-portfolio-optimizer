{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#These are the libraries you can use.  You may add any libraries directy related to threading if this is a direction\n",
    "#you wish to go (this is not from the course, so it's entirely on you if you wish to use threading).  Any\n",
    "#further libraries you wish to use you must email me, james@uwaterloo.ca, for permission.\n",
    "\n",
    "from IPython.display import display, Math, Latex\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numpy_financial as npf\n",
    "import yfinance as yf\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from datetime import datetime\n",
    "import scipy as sp \n",
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group Assignment\n",
    "### Team Number: 15\n",
    "### Team Member Names: Neil Zhang, Rahim Rehan, Krish Patel\n",
    "### Team Strategy Chosen: Risk-Free "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # CHANGE COMMMENTS BACK TO ORINGINAL\n",
    "# # \n",
    "\n",
    "# START_DATE = \"2024-10-01\" #used for volume calcs\n",
    "# END_DATE   = \"2025-10-01\" #used for volume calcs\n",
    "\n",
    "# def format_tickers(csv_file_path, ticker_column_name=\"Ticker\"):\n",
    "#     \"\"\"\n",
    "#     Reads a CSV of tickers, downloads data with yfinance, and returns\n",
    "#     one big DataFrame with:\n",
    "#       - only NYSE / TSX tickers\n",
    "#       - only tickers with avg daily volume >= 5000\n",
    "#       - only months that have at least 18 trading days\n",
    "#       - industry info attached to each row\n",
    "#     \"\"\"\n",
    "\n",
    "#     #Reads ticker list from inputed CSV\n",
    "#     data_table = pd.read_csv(csv_file_path)\n",
    "\n",
    "#     # choose which column contains the ticker symbols\n",
    "#     if ticker_column_name in data_table.columns:\n",
    "#         raw_ticker_column = data_table[ticker_column_name]\n",
    "#     else:\n",
    "#         # if the given name is not found, assume the first column has the tickers\n",
    "#         raw_ticker_column = data_table.iloc[:, 0]\n",
    "\n",
    "#     # build a clean Python list of tickers (remove NaN and empty strings)\n",
    "#     ticker_list = []\n",
    "#     for cell in raw_ticker_column:\n",
    "#         if pd.isna(cell) == False:\n",
    "#             ticker_text = str(cell)\n",
    "#             ticker_text = ticker_text.strip()\n",
    "#             if ticker_text != \"\" and (ticker_text not in ticker_list):\n",
    "#                 ticker_list.append(ticker_text)\n",
    "\n",
    "#     # Filtering through what meets requirments \n",
    "#     # this will store all rows for all valid tickers\n",
    "#     valid_tickers   = []   # first column\n",
    "#     valid_sectors   = []   # second column\n",
    "#     valid_currency  = []   # third column\n",
    "#     valid_marketcap = []   # fourth column \n",
    "\n",
    "#     # Processing each ticker one by one\n",
    "#     for ticker_symbol in ticker_list:\n",
    "\n",
    "#         is_valid_ticker = True\n",
    "\n",
    "#         yf_ticker_object = yf.Ticker(ticker_symbol)\n",
    "\n",
    "#         #Getting basic info for each stock(exchange and industry)\n",
    "#         ticker_info = yf_ticker_object.info\n",
    "#         fast_info   = yf_ticker_object.fast_info\n",
    "\n",
    "#         # grab sector info for the summary output\n",
    "#         sector_name = None\n",
    "#         if \"sector\" in ticker_info:\n",
    "#             sector_name = ticker_info[\"sector\"]\n",
    "\n",
    "#         # grab currency info for the summary output (USE info, NOT fast_info)\n",
    "#         currency_name = None\n",
    "#         if \"currency\" in ticker_info:\n",
    "#             currency_name = ticker_info[\"currency\"]\n",
    "#         elif \"financialCurrency\" in ticker_info:\n",
    "#             currency_name = ticker_info[\"financialCurrency\"]\n",
    "\n",
    "#         # keep only Canadian and US based tickers\n",
    "#         if (currency_name != \"CAD\") and (currency_name != \"USD\"):\n",
    "#             is_valid_ticker = False\n",
    "\n",
    "#         # get market cap, prefer fast_info; compute manually if needed\n",
    "#         market_cap = None\n",
    "\n",
    "#         # 1) direct from fast_info (this property is usually safe)\n",
    "#         try:\n",
    "#             if hasattr(fast_info, \"market_cap\"):\n",
    "#                 mc_value = fast_info.market_cap\n",
    "#                 if mc_value is not None:\n",
    "#                     market_cap = mc_value\n",
    "#         except Exception:\n",
    "#             # This only occurs if fast_info crashes, as sometimes it does not contain market cap\n",
    "#             pass \n",
    "\n",
    "#         # 2) compute manually if not there: last price * shares outstanding\n",
    "#         if market_cap is None:\n",
    "#             last_price = None\n",
    "#             if \"regularMarketPrice\" in ticker_info:\n",
    "#                 last_price = ticker_info[\"regularMarketPrice\"]\n",
    "\n",
    "#             shares_outstanding = None\n",
    "#             if \"sharesOutstanding\" in ticker_info:\n",
    "#                 shares_outstanding = ticker_info[\"sharesOutstanding\"]\n",
    "\n",
    "#             if (last_price is not None) and (shares_outstanding is not None):\n",
    "#                 market_cap = last_price * shares_outstanding\n",
    "\n",
    "#         # 3) final fallback: old .info marketCap if still missing\n",
    "#         if (market_cap is None) and (\"marketCap\" in ticker_info):\n",
    "#             market_cap = ticker_info[\"marketCap\"]\n",
    "\n",
    "#         # Downloading daily price + volume history\n",
    "#         if is_valid_ticker:\n",
    "#             price_history = yf_ticker_object.history(start=START_DATE, end=END_DATE)\n",
    "#         else:\n",
    "#             price_history = pd.DataFrame()\n",
    "\n",
    "#         # price_history must not be empty and must have a Volume column\n",
    "#         if is_valid_ticker:\n",
    "#             if (\"Volume\" not in price_history.columns) or (len(price_history) == 0):\n",
    "#                 is_valid_ticker = False\n",
    "\n",
    "#         # Dropping months with fewer than 18 trading days\n",
    "#         if is_valid_ticker:\n",
    "#             price_history = price_history.copy()\n",
    "\n",
    "#             # strip timezone from index first\n",
    "#             if price_history.index.tz is not None:\n",
    "#                 price_history.index = price_history.index.tz_localize(None)\n",
    "\n",
    "#             # Creating a \"Month\" column to help out later on in order to filter\n",
    "#             month_period_index = price_history.index.to_period(\"M\")\n",
    "#             price_history[\"Month\"] = month_period_index\n",
    "\n",
    "#             # Building list of unique months\n",
    "#             month_column = price_history[\"Month\"]\n",
    "#             unique_months = []\n",
    "#             for month_value in month_column:\n",
    "#                 if month_value not in unique_months:\n",
    "#                     unique_months.append(month_value)\n",
    "\n",
    "#             # find which months have at least 18 rows (trading days)\n",
    "#             months_with_enough_days = []\n",
    "#             for month_value in unique_months:\n",
    "#                 day_count = 0\n",
    "#                 for row_month in month_column:\n",
    "#                     if row_month == month_value:\n",
    "#                         day_count = day_count + 1\n",
    "#                 if day_count >= 18:\n",
    "#                     months_with_enough_days.append(month_value)\n",
    "\n",
    "#             # keep only rows whose Month is in months_with_enough_days\n",
    "#             keep_row_mask = price_history[\"Month\"].isin(months_with_enough_days)\n",
    "#             price_history = price_history[keep_row_mask]\n",
    "\n",
    "#             # if everything is dropped, ticker is no longer valid\n",
    "#             if len(price_history) == 0:\n",
    "#                 is_valid_ticker = False\n",
    "\n",
    "#         # Filter by average daily volume >= 5000\n",
    "#         if is_valid_ticker:\n",
    "#             average_daily_volume = price_history[\"Volume\"].mean()\n",
    "#             if average_daily_volume < 5000:\n",
    "#                 is_valid_ticker = False\n",
    "\n",
    "#         # If ticker passes all filters, add to final DataFrame\n",
    "#         if is_valid_ticker:\n",
    "#             valid_tickers.append(ticker_symbol)\n",
    "#             valid_sectors.append(sector_name)\n",
    "#             valid_currency.append(currency_name)\n",
    "#             valid_marketcap.append(market_cap)\n",
    "\n",
    "#     # Making everything clean and well sorted to output as return result\n",
    "#     if len(valid_tickers) > 0:\n",
    "#         all_tickers_data = pd.DataFrame({\n",
    "#             \"Ticker\":     valid_tickers,\n",
    "#             \"Sector\":     valid_sectors,\n",
    "#             \"Currency\":   valid_currency,\n",
    "#             \"MarketCap\":  valid_marketcap\n",
    "#         })\n",
    "#         all_tickers_data = all_tickers_data.reset_index(drop=True)\n",
    "#     else:\n",
    "#         all_tickers_data = pd.DataFrame(\n",
    "#             columns=[\"Ticker\", \"Sector\", \"Currency\", \"MarketCap\"]\n",
    "#         )\n",
    "\n",
    "#     return all_tickers_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # CHANGE COMMMENTS BACK TO ORINGINAL\n",
    "# # \n",
    "\n",
    "# START_DATE = \"2024-10-01\" #used for volume calcs\n",
    "# END_DATE   = \"2025-10-01\" #used for volume calcs\n",
    "\n",
    "# def format_tickers(csv_file_path, ticker_column_name=\"Ticker\"):\n",
    "#     \"\"\"\n",
    "#     Reads a CSV of tickers, downloads data with yfinance, and returns\n",
    "#     one big DataFrame with:\n",
    "#       - only tickers with avg daily volume >= 5000 (in the given period)\n",
    "#       - only tickers whose currency is CAD or USD\n",
    "#       - sector + market cap + currency attached to each row\n",
    "#     \"\"\"\n",
    "\n",
    "#     #Reads ticker list from inputed CSV\n",
    "#     data_table = pd.read_csv(csv_file_path)\n",
    "\n",
    "#     # choose which column contains the ticker symbols\n",
    "#     if ticker_column_name in data_table.columns:\n",
    "#         raw_ticker_column = data_table[ticker_column_name]\n",
    "#     else:\n",
    "#         # if the given name is not found, assume the first column has the tickers\n",
    "#         raw_ticker_column = data_table.iloc[:, 0]\n",
    "\n",
    "#     # build a clean Python list of tickers (remove NaN and empty strings)\n",
    "#     ticker_list = []\n",
    "#     for cell in raw_ticker_column:\n",
    "#         if pd.isna(cell) == False:\n",
    "#             ticker_text = str(cell)\n",
    "#             ticker_text = ticker_text.strip()\n",
    "#             if ticker_text != \"\" and (ticker_text not in ticker_list):\n",
    "#                 ticker_list.append(ticker_text)\n",
    "\n",
    "#     # Final lists for output\n",
    "#     valid_tickers   = []\n",
    "#     valid_sectors   = []\n",
    "#     valid_currency  = []\n",
    "#     valid_marketcap = []\n",
    "\n",
    "#     if len(ticker_list) == 0:\n",
    "#         return pd.DataFrame(columns=[\"Ticker\", \"Sector\", \"Currency\", \"MarketCap\"])\n",
    "\n",
    "#     # 1) BULK DOWNLOAD: one call to yf.download for all tickers\n",
    "#     price_history_all = yf.download(\n",
    "#         tickers=ticker_list,\n",
    "#         start=START_DATE,\n",
    "#         end=END_DATE,\n",
    "#         group_by=\"ticker\",   # columns grouped by ticker\n",
    "#         auto_adjust=False,\n",
    "#         threads=True\n",
    "#     )\n",
    "\n",
    "#     # 2) FILTER BY AVERAGE VOLUME >= 5000\n",
    "#     tickers_after_volume = []\n",
    "\n",
    "#     # price_history_all with group_by=\"ticker\" has a MultiIndex:\n",
    "#     # top level = ticker, second level = field (Open, High, Low, Close, Volume, ...)\n",
    "#     if isinstance(price_history_all.columns, pd.MultiIndex):\n",
    "#         for ticker_symbol in ticker_list:\n",
    "#             if ticker_symbol not in price_history_all.columns.get_level_values(0):\n",
    "#                 continue\n",
    "\n",
    "#             # get this ticker's data: columns = Open, High, Low, Close, Adj Close, Volume\n",
    "#             ticker_data = price_history_all[ticker_symbol].copy()\n",
    "\n",
    "#             if (\"Volume\" not in ticker_data.columns) or (len(ticker_data) == 0):\n",
    "#                 continue\n",
    "\n",
    "#             avg_volume = ticker_data[\"Volume\"].mean()\n",
    "#             if avg_volume >= 5000:\n",
    "#                 tickers_after_volume.append(ticker_symbol)\n",
    "#     else:\n",
    "#         # fallback if only one ticker and columns are not MultiIndex\n",
    "#         # treat the whole DataFrame as the first ticker\n",
    "#         if \"Volume\" in price_history_all.columns and len(price_history_all) > 0:\n",
    "#             avg_volume = price_history_all[\"Volume\"].mean()\n",
    "#             if avg_volume >= 5000 and len(ticker_list) > 0:\n",
    "#                 tickers_after_volume.append(ticker_list[0])\n",
    "\n",
    "#     # 3) FOR TICKERS THAT PASSED VOLUME FILTER:\n",
    "#     #    use fast_info to check currency (must be CAD or USD),\n",
    "#     #    and collect sector + market cap\n",
    "#     for ticker_symbol in tickers_after_volume:\n",
    "\n",
    "#         yf_ticker_object = yf.Ticker(ticker_symbol)\n",
    "\n",
    "#         # fast_info for currency + market cap\n",
    "#         try:\n",
    "#             fast_info = yf_ticker_object.fast_info\n",
    "#         except Exception:\n",
    "#             fast_info = None\n",
    "\n",
    "#         # default: assume invalid, only keep if checks pass\n",
    "#         is_valid_ticker = True\n",
    "\n",
    "#         # currency from fast_info first\n",
    "#         currency_name = None\n",
    "#         try:\n",
    "#             if (fast_info is not None) and hasattr(fast_info, \"currency\"):\n",
    "#                 currency_name = fast_info.currency\n",
    "#         except Exception:\n",
    "#             pass\n",
    "\n",
    "#         # if we still don't have currency, we can try .info as a fallback\n",
    "#         try:\n",
    "#             ticker_info = yf_ticker_object.info\n",
    "#         except Exception:\n",
    "#             ticker_info = {}\n",
    "\n",
    "#         if currency_name is None and isinstance(ticker_info, dict):\n",
    "#             if \"currency\" in ticker_info:\n",
    "#                 currency_name = ticker_info[\"currency\"]\n",
    "#             elif \"financialCurrency\" in ticker_info:\n",
    "#                 currency_name = ticker_info[\"financialCurrency\"]\n",
    "\n",
    "#         # keep only CAD / USD\n",
    "#         if (currency_name != \"CAD\") and (currency_name != \"USD\"):\n",
    "#             is_valid_ticker = False\n",
    "\n",
    "#         # sector from .info\n",
    "#         sector_name = None\n",
    "#         if isinstance(ticker_info, dict) and (\"sector\" in ticker_info):\n",
    "#             sector_name = ticker_info[\"sector\"]\n",
    "\n",
    "#         # market cap: prefer fast_info.market_cap, then fallback to info[\"marketCap\"]\n",
    "#         market_cap = None\n",
    "#         try:\n",
    "#             if (fast_info is not None) and hasattr(fast_info, \"market_cap\"):\n",
    "#                 mc_value = fast_info.market_cap\n",
    "#                 if mc_value is not None:\n",
    "#                     market_cap = mc_value\n",
    "#         except Exception:\n",
    "#             pass\n",
    "\n",
    "#         if (market_cap is None) and isinstance(ticker_info, dict) and (\"marketCap\" in ticker_info):\n",
    "#             market_cap = ticker_info[\"marketCap\"]\n",
    "\n",
    "#         if is_valid_ticker:\n",
    "#             valid_tickers.append(ticker_symbol)\n",
    "#             valid_sectors.append(sector_name)\n",
    "#             valid_currency.append(currency_name)\n",
    "#             valid_marketcap.append(market_cap)\n",
    "\n",
    "#     # 4) BUILD FINAL DATAFRAME: ticker, currency, market cap, sector\n",
    "#     if len(valid_tickers) > 0:\n",
    "#         all_tickers_data = pd.DataFrame({\n",
    "#             \"Ticker\":    valid_tickers,\n",
    "#             \"Sector\":    valid_sectors,\n",
    "#             \"Currency\":  valid_currency,\n",
    "#             \"MarketCap\": valid_marketcap\n",
    "#         })\n",
    "#         all_tickers_data = all_tickers_data.reset_index(drop=True)\n",
    "#     else:\n",
    "#         all_tickers_data = pd.DataFrame(\n",
    "#             columns=[\"Ticker\", \"Sector\", \"Currency\", \"MarketCap\"]\n",
    "#         )\n",
    "\n",
    "#     return all_tickers_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "START_DATE = \"2024-10-01\"\n",
    "END_DATE   = \"2025-10-01\"\n",
    "\n",
    "def format_tickers(csv_file_path, ticker_column_name=\"Ticker\"):\n",
    "\n",
    "    data_table = pd.read_csv(csv_file_path)\n",
    "\n",
    "    # clean ticker list\n",
    "    if ticker_column_name in data_table.columns:\n",
    "        raw_col = data_table[ticker_column_name]\n",
    "    else:\n",
    "        raw_col = data_table.iloc[:, 0]\n",
    "\n",
    "    ticker_list = []\n",
    "    for cell in raw_col:\n",
    "        if pd.isna(cell) == False:\n",
    "            t = str(cell).strip()\n",
    "            if t != \"\" and (t not in ticker_list):\n",
    "                ticker_list.append(t)\n",
    "\n",
    "    if len(ticker_list) == 0:\n",
    "        return pd.DataFrame(columns=[\"Ticker\",\"Sector\",\"Currency\",\"MarketCap\"])\n",
    "\n",
    "    # -------------------------\n",
    "    # 1) BULK DOWNLOAD HISTORY\n",
    "    # -------------------------\n",
    "    hist = yf.download(\n",
    "        tickers=ticker_list,\n",
    "        start=START_DATE,\n",
    "        end=END_DATE,\n",
    "        group_by=\"ticker\",\n",
    "        auto_adjust=False,\n",
    "        threads=True\n",
    "    )\n",
    "\n",
    "    # -------------------------\n",
    "    # 2) FILTER BY AVG VOLUME\n",
    "    # -------------------------\n",
    "    tickers_after_vol = []\n",
    "\n",
    "    if isinstance(hist.columns, pd.MultiIndex):\n",
    "        # multi-ticker case\n",
    "        for sym in ticker_list:\n",
    "            if sym not in hist.columns.get_level_values(0):\n",
    "                continue\n",
    "\n",
    "            df = hist[sym]\n",
    "            if \"Volume\" not in df.columns or len(df) == 0:\n",
    "                continue\n",
    "\n",
    "            if df[\"Volume\"].mean() >= 5000:\n",
    "                tickers_after_vol.append(sym)\n",
    "    else:\n",
    "        # single ticker fallback\n",
    "        if \"Volume\" in hist.columns and len(hist) > 0:\n",
    "            if hist[\"Volume\"].mean() >= 5000:\n",
    "                tickers_after_vol.append(ticker_list[0])\n",
    "\n",
    "    # -------------------------------------------------------\n",
    "    # 3) THREADING FUNCTION: fetch fast_info / info per ticker\n",
    "    # -------------------------------------------------------\n",
    "    def process_ticker(sym):\n",
    "        \"\"\"Fetch currency, sector, market cap for one ticker (threaded).\"\"\"\n",
    "        try:\n",
    "            obj = yf.Ticker(sym)\n",
    "\n",
    "            # fast_info\n",
    "            try:\n",
    "                fi = obj.fast_info\n",
    "            except:\n",
    "                fi = None\n",
    "\n",
    "            # full info\n",
    "            try:\n",
    "                info = obj.info\n",
    "            except:\n",
    "                info = {}\n",
    "\n",
    "            # currency\n",
    "            currency = None\n",
    "            if fi is not None and hasattr(fi, \"currency\"):\n",
    "                currency = fi.currency\n",
    "\n",
    "            if currency is None and \"currency\" in info:\n",
    "                currency = info[\"currency\"]\n",
    "            elif currency is None and \"financialCurrency\" in info:\n",
    "                currency = info[\"financialCurrency\"]\n",
    "\n",
    "            # must be USD or CAD\n",
    "            if currency not in [\"USD\", \"CAD\"]:\n",
    "                return None\n",
    "\n",
    "            # sector\n",
    "            sector = info.get(\"sector\", None)\n",
    "\n",
    "            # market cap\n",
    "            mc = None\n",
    "            if fi is not None and hasattr(fi, \"market_cap\"):\n",
    "                mc = fi.market_cap\n",
    "            if mc is None:\n",
    "                mc = info.get(\"marketCap\", None)\n",
    "\n",
    "            return (sym, sector, currency, mc)\n",
    "\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "    # -------------------------------------\n",
    "    # 4) MULTITHREAD the info-fetching part\n",
    "    # -------------------------------------\n",
    "    results = []\n",
    "    with ThreadPoolExecutor(max_workers=15) as executor:\n",
    "        futures = {executor.submit(process_ticker, sym): sym for sym in tickers_after_vol}\n",
    "\n",
    "        for fut in as_completed(futures):\n",
    "            res = fut.result()\n",
    "            if res is not None:\n",
    "                results.append(res)\n",
    "\n",
    "    # -------------------------\n",
    "    # 5) Build DataFrame output\n",
    "    # -------------------------\n",
    "    if len(results) == 0:\n",
    "        return pd.DataFrame(columns=[\"Ticker\",\"Sector\",\"Currency\",\"MarketCap\"])\n",
    "\n",
    "    df = pd.DataFrame(results, columns=[\"Ticker\",\"Sector\",\"Currency\",\"MarketCap\"])\n",
    "    return df.reset_index(drop=True)\n",
    "\n",
    "test = format_tickers(\"Extended_Tickers_Example.csv\")\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  140 of 140 completed\n",
      "\n",
      "9 Failed downloads:\n",
      "['PTR', 'AGN', 'BRK.B', 'GIB.A.TO', 'ATVI', 'DFS', 'MON', 'CELG', 'RTN']: YFTzMissingError('possibly delisted; no timezone found')\n",
      "/var/folders/t6/0pgnx1h16112t2cmg14zf9ph0000gn/T/ipykernel_83480/2667213605.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  weekly_closes[f'Close {i}'] = prices\n",
      "/var/folders/t6/0pgnx1h16112t2cmg14zf9ph0000gn/T/ipykernel_83480/2667213605.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  weekly_closes[f'Close {i}'] = prices\n",
      "/var/folders/t6/0pgnx1h16112t2cmg14zf9ph0000gn/T/ipykernel_83480/2667213605.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  weekly_closes[f'Close {i}'] = prices\n",
      "/var/folders/t6/0pgnx1h16112t2cmg14zf9ph0000gn/T/ipykernel_83480/2667213605.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  weekly_closes[f'Close {i}'] = prices\n",
      "/var/folders/t6/0pgnx1h16112t2cmg14zf9ph0000gn/T/ipykernel_83480/2667213605.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  weekly_closes[f'Close {i}'] = prices\n",
      "/var/folders/t6/0pgnx1h16112t2cmg14zf9ph0000gn/T/ipykernel_83480/2667213605.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  weekly_closes[f'Close {i}'] = prices\n",
      "/var/folders/t6/0pgnx1h16112t2cmg14zf9ph0000gn/T/ipykernel_83480/2667213605.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  weekly_closes[f'Close {i}'] = prices\n",
      "/var/folders/t6/0pgnx1h16112t2cmg14zf9ph0000gn/T/ipykernel_83480/2667213605.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  weekly_closes[f'Close {i}'] = prices\n",
      "/var/folders/t6/0pgnx1h16112t2cmg14zf9ph0000gn/T/ipykernel_83480/2667213605.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  weekly_closes[f'Close {i}'] = prices\n",
      "/var/folders/t6/0pgnx1h16112t2cmg14zf9ph0000gn/T/ipykernel_83480/2667213605.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  weekly_closes[f'Close {i}'] = prices\n",
      "/var/folders/t6/0pgnx1h16112t2cmg14zf9ph0000gn/T/ipykernel_83480/2667213605.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  weekly_closes[f'Close {i}'] = prices\n",
      "/var/folders/t6/0pgnx1h16112t2cmg14zf9ph0000gn/T/ipykernel_83480/2667213605.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  weekly_closes[f'Close {i}'] = prices\n",
      "/var/folders/t6/0pgnx1h16112t2cmg14zf9ph0000gn/T/ipykernel_83480/2667213605.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  weekly_closes[f'Close {i}'] = prices\n",
      "/var/folders/t6/0pgnx1h16112t2cmg14zf9ph0000gn/T/ipykernel_83480/2667213605.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  weekly_closes[f'Close {i}'] = prices\n",
      "/var/folders/t6/0pgnx1h16112t2cmg14zf9ph0000gn/T/ipykernel_83480/2667213605.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  weekly_closes[f'Close {i}'] = prices\n",
      "/var/folders/t6/0pgnx1h16112t2cmg14zf9ph0000gn/T/ipykernel_83480/2667213605.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  weekly_closes[f'Close {i}'] = prices\n",
      "/var/folders/t6/0pgnx1h16112t2cmg14zf9ph0000gn/T/ipykernel_83480/2667213605.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  weekly_closes[f'Close {i}'] = prices\n",
      "/var/folders/t6/0pgnx1h16112t2cmg14zf9ph0000gn/T/ipykernel_83480/2667213605.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  weekly_closes[f'Close {i}'] = prices\n",
      "/var/folders/t6/0pgnx1h16112t2cmg14zf9ph0000gn/T/ipykernel_83480/2667213605.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  weekly_closes[f'Close {i}'] = prices\n",
      "/var/folders/t6/0pgnx1h16112t2cmg14zf9ph0000gn/T/ipykernel_83480/2667213605.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  weekly_closes[f'Close {i}'] = prices\n",
      "/var/folders/t6/0pgnx1h16112t2cmg14zf9ph0000gn/T/ipykernel_83480/2667213605.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  weekly_closes[f'Close {i}'] = prices\n",
      "/var/folders/t6/0pgnx1h16112t2cmg14zf9ph0000gn/T/ipykernel_83480/2667213605.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  weekly_closes[f'Close {i}'] = prices\n",
      "/var/folders/t6/0pgnx1h16112t2cmg14zf9ph0000gn/T/ipykernel_83480/2667213605.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  weekly_closes[f'Close {i}'] = prices\n",
      "/var/folders/t6/0pgnx1h16112t2cmg14zf9ph0000gn/T/ipykernel_83480/2667213605.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  weekly_closes[f'Close {i}'] = prices\n",
      "/var/folders/t6/0pgnx1h16112t2cmg14zf9ph0000gn/T/ipykernel_83480/2667213605.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  weekly_closes[f'Close {i}'] = prices\n",
      "/var/folders/t6/0pgnx1h16112t2cmg14zf9ph0000gn/T/ipykernel_83480/2667213605.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  weekly_closes[f'Close {i}'] = prices\n",
      "/var/folders/t6/0pgnx1h16112t2cmg14zf9ph0000gn/T/ipykernel_83480/2667213605.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  weekly_closes[f'Close {i}'] = prices\n",
      "/var/folders/t6/0pgnx1h16112t2cmg14zf9ph0000gn/T/ipykernel_83480/2667213605.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  weekly_closes[f'Close {i}'] = prices\n",
      "/var/folders/t6/0pgnx1h16112t2cmg14zf9ph0000gn/T/ipykernel_83480/2667213605.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  weekly_closes[f'Close {i}'] = prices\n",
      "/var/folders/t6/0pgnx1h16112t2cmg14zf9ph0000gn/T/ipykernel_83480/2667213605.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  weekly_closes[f'Close {i}'] = prices\n",
      "/var/folders/t6/0pgnx1h16112t2cmg14zf9ph0000gn/T/ipykernel_83480/2667213605.py:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  percent_change[f'% Change {col_name}'] = closes[i].pct_change(fill_method=None) * 100\n",
      "/var/folders/t6/0pgnx1h16112t2cmg14zf9ph0000gn/T/ipykernel_83480/2667213605.py:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  percent_change[f'% Change {col_name}'] = closes[i].pct_change(fill_method=None) * 100\n",
      "/var/folders/t6/0pgnx1h16112t2cmg14zf9ph0000gn/T/ipykernel_83480/2667213605.py:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  percent_change[f'% Change {col_name}'] = closes[i].pct_change(fill_method=None) * 100\n",
      "/var/folders/t6/0pgnx1h16112t2cmg14zf9ph0000gn/T/ipykernel_83480/2667213605.py:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  percent_change[f'% Change {col_name}'] = closes[i].pct_change(fill_method=None) * 100\n",
      "/var/folders/t6/0pgnx1h16112t2cmg14zf9ph0000gn/T/ipykernel_83480/2667213605.py:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  percent_change[f'% Change {col_name}'] = closes[i].pct_change(fill_method=None) * 100\n",
      "/var/folders/t6/0pgnx1h16112t2cmg14zf9ph0000gn/T/ipykernel_83480/2667213605.py:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  percent_change[f'% Change {col_name}'] = closes[i].pct_change(fill_method=None) * 100\n",
      "/var/folders/t6/0pgnx1h16112t2cmg14zf9ph0000gn/T/ipykernel_83480/2667213605.py:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  percent_change[f'% Change {col_name}'] = closes[i].pct_change(fill_method=None) * 100\n",
      "/var/folders/t6/0pgnx1h16112t2cmg14zf9ph0000gn/T/ipykernel_83480/2667213605.py:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  percent_change[f'% Change {col_name}'] = closes[i].pct_change(fill_method=None) * 100\n",
      "/var/folders/t6/0pgnx1h16112t2cmg14zf9ph0000gn/T/ipykernel_83480/2667213605.py:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  percent_change[f'% Change {col_name}'] = closes[i].pct_change(fill_method=None) * 100\n",
      "/var/folders/t6/0pgnx1h16112t2cmg14zf9ph0000gn/T/ipykernel_83480/2667213605.py:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  percent_change[f'% Change {col_name}'] = closes[i].pct_change(fill_method=None) * 100\n",
      "/var/folders/t6/0pgnx1h16112t2cmg14zf9ph0000gn/T/ipykernel_83480/2667213605.py:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  percent_change[f'% Change {col_name}'] = closes[i].pct_change(fill_method=None) * 100\n",
      "/var/folders/t6/0pgnx1h16112t2cmg14zf9ph0000gn/T/ipykernel_83480/2667213605.py:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  percent_change[f'% Change {col_name}'] = closes[i].pct_change(fill_method=None) * 100\n",
      "/var/folders/t6/0pgnx1h16112t2cmg14zf9ph0000gn/T/ipykernel_83480/2667213605.py:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  percent_change[f'% Change {col_name}'] = closes[i].pct_change(fill_method=None) * 100\n",
      "/var/folders/t6/0pgnx1h16112t2cmg14zf9ph0000gn/T/ipykernel_83480/2667213605.py:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  percent_change[f'% Change {col_name}'] = closes[i].pct_change(fill_method=None) * 100\n",
      "/var/folders/t6/0pgnx1h16112t2cmg14zf9ph0000gn/T/ipykernel_83480/2667213605.py:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  percent_change[f'% Change {col_name}'] = closes[i].pct_change(fill_method=None) * 100\n",
      "/var/folders/t6/0pgnx1h16112t2cmg14zf9ph0000gn/T/ipykernel_83480/2667213605.py:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  percent_change[f'% Change {col_name}'] = closes[i].pct_change(fill_method=None) * 100\n",
      "/var/folders/t6/0pgnx1h16112t2cmg14zf9ph0000gn/T/ipykernel_83480/2667213605.py:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  percent_change[f'% Change {col_name}'] = closes[i].pct_change(fill_method=None) * 100\n",
      "/var/folders/t6/0pgnx1h16112t2cmg14zf9ph0000gn/T/ipykernel_83480/2667213605.py:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  percent_change[f'% Change {col_name}'] = closes[i].pct_change(fill_method=None) * 100\n",
      "/var/folders/t6/0pgnx1h16112t2cmg14zf9ph0000gn/T/ipykernel_83480/2667213605.py:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  percent_change[f'% Change {col_name}'] = closes[i].pct_change(fill_method=None) * 100\n",
      "/var/folders/t6/0pgnx1h16112t2cmg14zf9ph0000gn/T/ipykernel_83480/2667213605.py:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  percent_change[f'% Change {col_name}'] = closes[i].pct_change(fill_method=None) * 100\n",
      "/var/folders/t6/0pgnx1h16112t2cmg14zf9ph0000gn/T/ipykernel_83480/2667213605.py:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  percent_change[f'% Change {col_name}'] = closes[i].pct_change(fill_method=None) * 100\n",
      "/var/folders/t6/0pgnx1h16112t2cmg14zf9ph0000gn/T/ipykernel_83480/2667213605.py:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  percent_change[f'% Change {col_name}'] = closes[i].pct_change(fill_method=None) * 100\n",
      "/var/folders/t6/0pgnx1h16112t2cmg14zf9ph0000gn/T/ipykernel_83480/2667213605.py:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  percent_change[f'% Change {col_name}'] = closes[i].pct_change(fill_method=None) * 100\n",
      "/var/folders/t6/0pgnx1h16112t2cmg14zf9ph0000gn/T/ipykernel_83480/2667213605.py:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  percent_change[f'% Change {col_name}'] = closes[i].pct_change(fill_method=None) * 100\n",
      "/var/folders/t6/0pgnx1h16112t2cmg14zf9ph0000gn/T/ipykernel_83480/2667213605.py:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  percent_change[f'% Change {col_name}'] = closes[i].pct_change(fill_method=None) * 100\n",
      "/var/folders/t6/0pgnx1h16112t2cmg14zf9ph0000gn/T/ipykernel_83480/2667213605.py:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  percent_change[f'% Change {col_name}'] = closes[i].pct_change(fill_method=None) * 100\n",
      "/var/folders/t6/0pgnx1h16112t2cmg14zf9ph0000gn/T/ipykernel_83480/2667213605.py:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  percent_change[f'% Change {col_name}'] = closes[i].pct_change(fill_method=None) * 100\n",
      "/var/folders/t6/0pgnx1h16112t2cmg14zf9ph0000gn/T/ipykernel_83480/2667213605.py:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  percent_change[f'% Change {col_name}'] = closes[i].pct_change(fill_method=None) * 100\n",
      "/var/folders/t6/0pgnx1h16112t2cmg14zf9ph0000gn/T/ipykernel_83480/2667213605.py:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  percent_change[f'% Change {col_name}'] = closes[i].pct_change(fill_method=None) * 100\n",
      "/var/folders/t6/0pgnx1h16112t2cmg14zf9ph0000gn/T/ipykernel_83480/2667213605.py:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  percent_change[f'% Change {col_name}'] = closes[i].pct_change(fill_method=None) * 100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n# Access each piece like:\\ndisplay(covariance_matrix['Covariance'])\\ndisplay(covariance_matrix['Std_Dev'])\\n\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Krish, Info Extraction \n",
    "\n",
    "returns_start = \"2024-11-14\"\n",
    "returns_end = \"2025-11-14\"\n",
    "\n",
    "# Function to return a list of all tickers (first column elements)\n",
    "def get_ticker_list (tickers_df):\n",
    "     return tickers_df.iloc[:, 0].tolist()\n",
    "\n",
    "# Gets weekly closes of all the stocks in a list of tickers\n",
    "def get_weekly_closes (ticker_lst, start_date, end_date):\n",
    "    #Define a dataframe to hold weekly close prices (checks every friday)\n",
    "    weekly_closes = pd.DataFrame()\n",
    "    #Extract the weekly close prices and store them in the dataframe\n",
    "    for i in ticker_lst:\n",
    "        ticker = yf.Ticker(i)\n",
    "        data = ticker.history(start=start_date, end=end_date)\n",
    "        data.index = pd.to_datetime(data.index) # ensure datetime index\n",
    "        #last() takes the last trading price of the week\n",
    "        prices = data['Close'].resample('W-FRI').last()\n",
    "        weekly_closes[f'Close {i}'] = prices\n",
    "    #Strip time\n",
    "    weekly_closes.index = weekly_closes.index.strftime('%Y-%m-%d')\n",
    "    return weekly_closes\n",
    "\n",
    "# Creates a df with the (weekly) %change for each column\n",
    "def get_percent_change (closes, start_date, end_date):\n",
    "    percent_change = pd.DataFrame()\n",
    "    for i in closes:\n",
    "        col_name = i[6:]\n",
    "        #fill_method=None to deal with delisted stocks\n",
    "        percent_change[f'% Change {col_name}'] = closes[i].pct_change(fill_method=None) * 100\n",
    "    return percent_change\n",
    "\n",
    "# Calculate covariance, correlation, variance, standard deviation\n",
    "def get_calculations(ticker_list, start_date, end_date):\n",
    "    weekly_closes = get_weekly_closes(ticker_list, start_date, end_date)\n",
    "    weekly_percent_change = get_percent_change(weekly_closes, start_date, end_date)\n",
    "    covariance_matrix = {\n",
    "        'Covariance': weekly_percent_change.cov(),\n",
    "        'Correlation': weekly_percent_change.corr(),\n",
    "        'Variance': weekly_percent_change.var(),\n",
    "        'Std_Dev': weekly_percent_change.std()}\n",
    "    return covariance_matrix\n",
    "\n",
    "info_df = format_tickers(\"Extended_Tickers_Example.csv\")\n",
    "ticker_list = (get_ticker_list(info_df)) # List of all tickers\n",
    "primary_calculations = get_calculations(ticker_list, returns_start, returns_end)\n",
    "\"\"\"\n",
    "# Access each piece like:\n",
    "display(covariance_matrix['Covariance'])\n",
    "display(covariance_matrix['Std_Dev'])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neil opimization models \n",
    "# This is the function we want to minimize, aka the minimum variance function\n",
    "def port_variance(weights, cov_matrix):\n",
    "    \"\"\"     \n",
    "    port_variance is the function that calculates the variance of a portfolio. It performs \n",
    "    dot product/matrix multiplication on the weights and covariance matrixes. \n",
    "\n",
    "    :param weights: an array that represents the weight of each asset\n",
    "    :param cov_matrix: a 2D matrix that represents the covariance between each asset \n",
    "    :return: variance of the portfolio\n",
    "    \"\"\"\n",
    "    weights_col = weights.reshape(-1, 1) # Turns into column vector\n",
    "    port_var = np.dot(weights_col.transpose(), (np.dot(cov_matrix, weights_col))) # Doing dot product \n",
    "    return port_var[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code is what runs the primary function\n",
    "\n",
    "# Primary minimization, there is bounds in this \n",
    "def primary_minimization(cov_matrix):\n",
    "    \"\"\"     \n",
    "    primary_minimization is the function that finds the weightings that result in the mimimum variance. \n",
    "    It performs this using scipy optimization. This perimary version does not consider bounds. \n",
    "\n",
    "    :param cov_matrix: a 2D matrix that represents the covariance between each asset \n",
    "    :return: returns the minimum variance and the weightings associated with that\n",
    "    \"\"\"\n",
    "    num_assets = cov_matrix.shape[0]\n",
    "    initial_weight = [1/num_assets] * num_assets # The initial guess of the weights\n",
    "\n",
    "    constraint = {\n",
    "        'type':'eq', # Constraint type is equality\n",
    "        'fun': lambda w: sum(w) - 1 # The function's weight's must sum to 1\n",
    "        }\n",
    "    \n",
    "    weight_bounds = [(0, 1)] * num_assets # Does not allow short selling\n",
    "    \n",
    "    # Finds the resilt of the minimization of the port_variance function, using the initial guess, keeping the cov_matrix constant using the SLSQP method, and with the above listed constraint\n",
    "    result = minimize(fun=port_variance, x0=initial_weight, args=(cov_matrix,), method='SLSQP', bounds=weight_bounds, constraints=constraint)\n",
    "    return result.fun, result.x\n",
    "\n",
    "pd_cov_matrix = primary_calculations['Covariance']\n",
    "numpy_cov_matrix = pd_cov_matrix.to_numpy() # Matrix of covariances of assets\n",
    "primary_var, primary_weights = primary_minimization(numpy_cov_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Secondary Optimization logic: Let n be the size of the tickers, we now have an optimized weighting for those n stocks\n",
    "We want to find the most optimal set up of 10-25 stocks out of those n. To do so we will try every combination, however if we wanted to brute force from those n stocks it'd take an absurd amount of compute. Instead we will take into the fact that we have the weightings of the (unconstrained) optimization. The higher the weighting of an asset, the more important it is to the minimizing the  variance, thus we will sort the n-optimized assets from highest to lowest weighting. Starting from the top we will then build a 10-25 asset size portfolio and calculate the variance of each portfolio, finding the one with the least variance. \n",
    "\n",
    "We must also consider the fact that each portfolio has restraints, aka the min/max weighting of one stock, the max amount of sectors, and the mkt caps\n",
    "In regards to the weighting rules, those can be implemented via scipy's minimization constraints, making all weightings are in a certain range\n",
    "In regards to the max amount of sectors, when building the portfolios we will keep count of the sectors, if any sector exceeds a certain amount such that are over represented, we skip an asset and move on\n",
    "In regards to the small and large mkt cap, we can check after building the portfolio, if one of them is missing we can delete the lowest weighted (least important) asset and then add in a new asset from the list that satisfies the missing mkt cap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The secondary optimization that includes the bounds \n",
    "def secondary_minimization(cov_matrix):\n",
    "    \"\"\"     \n",
    "    secondary_minimization is the function that finds the weightings that result in the mimimum variance while considering the bounds.\n",
    "    That is, it ensures the weightings \n",
    "\n",
    "    :param cov_matrix: a 2D matrix that represents the covariance between each asset \n",
    "    :return: returns the minimum variance and the weightings associated with that\n",
    "    \"\"\"\n",
    "    num_assets = len(cov_matrix[0]) \n",
    "    initial_weight = [1/num_assets] * num_assets # The initial guess of the weights\n",
    "\n",
    "    min_weight = (100/2*num_assets)/100 # Do not need to include portfolio value, because 1 is the portfolio value (and sum of weights)\n",
    "    max_weight = 0.15 # Same as above\n",
    "\n",
    "    constraint = {\n",
    "        'type':'eq', # Constraint type is equality\n",
    "        'fun': lambda w: sum(w) - 1 # The function's weight's must sum to 1\n",
    "        }\n",
    "    \n",
    "    weight_bounds = [(min_weight, max_weight)] * num_assets\n",
    "    \n",
    "    # Finds the resilt of the minimization of the port_variance function, using the initial guess, keeping the cov_matrix constant using the SLSQP method, and with the above listed constraint\n",
    "    result = minimize(fun=port_variance, x0=initial_weight, args=(cov_matrix,), method='SLSQP', bounds = weight_bounds, constraints=constraint)\n",
    "    return result.fun, result.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ticker</th>\n",
       "      <th>Sector</th>\n",
       "      <th>Currency</th>\n",
       "      <th>MarketCap</th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BNS.TO</td>\n",
       "      <td>Financial Services</td>\n",
       "      <td>CAD</td>\n",
       "      <td>1.171217e+11</td>\n",
       "      <td>1.536950e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CME</td>\n",
       "      <td>Financial Services</td>\n",
       "      <td>USD</td>\n",
       "      <td>9.825983e+10</td>\n",
       "      <td>1.150585e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>EXC</td>\n",
       "      <td>Utilities</td>\n",
       "      <td>USD</td>\n",
       "      <td>4.601502e+10</td>\n",
       "      <td>9.665896e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AEP</td>\n",
       "      <td>Utilities</td>\n",
       "      <td>USD</td>\n",
       "      <td>6.514718e+10</td>\n",
       "      <td>8.973519e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DG</td>\n",
       "      <td>Consumer Defensive</td>\n",
       "      <td>USD</td>\n",
       "      <td>2.193357e+10</td>\n",
       "      <td>8.773489e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>MS</td>\n",
       "      <td>Financial Services</td>\n",
       "      <td>USD</td>\n",
       "      <td>2.590693e+11</td>\n",
       "      <td>2.959844e-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>LOW</td>\n",
       "      <td>Consumer Cyclical</td>\n",
       "      <td>USD</td>\n",
       "      <td>1.280980e+11</td>\n",
       "      <td>2.765802e-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>FDX</td>\n",
       "      <td>Industrials</td>\n",
       "      <td>USD</td>\n",
       "      <td>6.195483e+10</td>\n",
       "      <td>2.698987e-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>MDT</td>\n",
       "      <td>Healthcare</td>\n",
       "      <td>USD</td>\n",
       "      <td>1.286977e+11</td>\n",
       "      <td>1.536729e-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>CL</td>\n",
       "      <td>Consumer Defensive</td>\n",
       "      <td>USD</td>\n",
       "      <td>6.342109e+10</td>\n",
       "      <td>8.849463e-17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>130 rows  5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Ticker              Sector Currency     MarketCap        weight\n",
       "0    BNS.TO  Financial Services      CAD  1.171217e+11  1.536950e-01\n",
       "1       CME  Financial Services      USD  9.825983e+10  1.150585e-01\n",
       "2       EXC           Utilities      USD  4.601502e+10  9.665896e-02\n",
       "3       AEP           Utilities      USD  6.514718e+10  8.973519e-02\n",
       "4        DG  Consumer Defensive      USD  2.193357e+10  8.773489e-02\n",
       "..      ...                 ...      ...           ...           ...\n",
       "125      MS  Financial Services      USD  2.590693e+11  2.959844e-16\n",
       "126     LOW   Consumer Cyclical      USD  1.280980e+11  2.765802e-16\n",
       "127     FDX         Industrials      USD  6.195483e+10  2.698987e-16\n",
       "128     MDT          Healthcare      USD  1.286977e+11  1.536729e-16\n",
       "129      CL  Consumer Defensive      USD  6.342109e+10  8.849463e-17\n",
       "\n",
       "[130 rows x 5 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info_df['weight'] = primary_weights #Assume info_df holds the tickers, sector, market cap, etc and not the dates etc. We now add the weights.\n",
    "ordered_info_df = info_df.copy().sort_values('weight', ascending = False).reset_index(drop=True)\n",
    "\n",
    "ordered_info_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code that creates a portfolio that matches the requirements\n",
    "\n",
    "# Determines all the indexes in a portfolio that are large cap stocks\n",
    "def is_lg_cap(portfolio):\n",
    "    lg_cap = []\n",
    "    for i in range(len(portfolio)):\n",
    "        if portfolio[i]['MarketCap'] > 10_000_000_000:\n",
    "            lg_cap.append(i)\n",
    "    return lg_cap\n",
    "\n",
    "# Determines all the indexes in a portfolio that are small cap stocks\n",
    "def is_sm_cap(portfolio):\n",
    "    sm_cap = []\n",
    "    for i in range(len(portfolio)):\n",
    "        if portfolio[i]['MarketCap'] < 2_000_000_000:\n",
    "            sm_cap.append(i)\n",
    "    return sm_cap\n",
    "\n",
    "# Determines if an individual stock is a large market cap\n",
    "def is_lg(row):\n",
    "    return row['MarketCap'] > 10_000_000_000\n",
    "\n",
    "# Determines if an individual stock is a small market cap\n",
    "def is_sm(row):\n",
    "    return row['MarketCap'] < 2_000_000_000\n",
    "\n",
    "# Determines the index of the least important stock. Least important in this case is the one with the lowest weighting in the ordered list, but\n",
    "# if the least important is either the ONLY SMALL or LARGE cap stock then the second least important stock is now designated the least important\n",
    "def find_least_imp(portfolio, lg_indxs, sm_indxs):\n",
    "    \"\"\" \n",
    "    :param portfolio: A list of stock data in Series format\n",
    "    :param lg_indxs: A list of all indexes that hold large cap stocks\n",
    "    :param sm_indxs: A list of all indexes that hold small cap stocks\n",
    "    :return: integer that represents the index that is desginated least important\n",
    "    \"\"\"\n",
    "\n",
    "    only_large_idx = None\n",
    "    only_small_idx = None\n",
    "\n",
    "    # Determines the protected indexes\n",
    "    if len(lg_indxs) == 1:\n",
    "        only_large_idx = lg_indxs[0]\n",
    "    if len(sm_indxs) == 1:\n",
    "        only_small_idx = sm_indxs[0]\n",
    "\n",
    "    for i in range(len(portfolio) - 1, -1, -1):\n",
    "        if i != only_large_idx and i != only_small_idx:\n",
    "            return i\n",
    "\n",
    "# Creates a portfolio that is valid \n",
    "def create_portfolio(size):\n",
    "    \"\"\" \n",
    "    :param size: the size of the portfolio\n",
    "    :return: a list of tickers that are in the portfolio \n",
    "    \"\"\"\n",
    "\n",
    "    portfolio = []\n",
    "    port_sectors = []\n",
    "    i = 0\n",
    "    max_sector_num = int(size * 0.4)\n",
    "    ticker_only = []\n",
    "    \n",
    "    # Creates preliminary portfolio \n",
    "    while len(portfolio) < size:\n",
    "        cur = ordered_info_df.iloc[i]\n",
    "        cur_sector = cur['Sector']\n",
    "\n",
    "        # Ensures that no sector is above max weight\n",
    "        if (port_sectors.count(cur_sector) < max_sector_num):\n",
    "            portfolio.append(cur)\n",
    "            port_sectors.append(cur_sector)\n",
    "        i += 1\n",
    "\n",
    "    lg_idxs = is_lg_cap(portfolio)\n",
    "    sm_idxs = is_sm_cap(portfolio)\n",
    "\n",
    "    # Fixes the no large market cap issue\n",
    "    while len(lg_idxs) == 0: \n",
    "        replaced = find_least_imp(portfolio, lg_idxs, sm_idxs)\n",
    "        cur = ordered_info_df.iloc[i]\n",
    "        cur_sector = cur['Sector']\n",
    "\n",
    "        temp_sectors = port_sectors.copy()\n",
    "        temp_sectors.pop(replaced)\n",
    "        \n",
    "        if (is_lg(cur)) and (temp_sectors.count(cur_sector) < max_sector_num):\n",
    "            portfolio[replaced] = cur\n",
    "            port_sectors[replaced] = cur_sector\n",
    "\n",
    "            lg_idxs = is_lg_cap(portfolio)\n",
    "            sm_idxs = is_sm_cap(portfolio)\n",
    "        i += 1\n",
    "\n",
    "    # Fixes the no small market cap issues\n",
    "    while len(sm_idxs) == 0: \n",
    "        replaced = find_least_imp(portfolio, lg_idxs, sm_idxs)\n",
    "        cur = ordered_info_df.iloc[i]\n",
    "        cur_sector = cur['Sector']\n",
    "\n",
    "        temp_sectors = port_sectors.copy()\n",
    "        temp_sectors.pop(replaced)\n",
    "        \n",
    "        if (is_sm(cur)) and (temp_sectors.count(cur_sector) < max_sector_num):\n",
    "            portfolio[replaced] = cur\n",
    "            port_sectors[replaced] = cur_sector\n",
    "\n",
    "            lg_idxs = is_lg_cap(portfolio)\n",
    "            sm_idxs = is_sm_cap(portfolio)\n",
    "        i += 1\n",
    "    \n",
    "    for stock in portfolio:\n",
    "        ticker_name = stock[\"Ticker\"]\n",
    "        ticker_only.append(ticker_name)\n",
    "    \n",
    "    return ticker_only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "single positional indexer is out-of-bounds",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m (count \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m10\u001b[39m) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m26\u001b[39m:\n\u001b[0;32m----> 8\u001b[0m     all_ports\u001b[38;5;241m.\u001b[39mappend(create_portfolio(count\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m10\u001b[39m))\n\u001b[1;32m     10\u001b[0m     temp_cov \u001b[38;5;241m=\u001b[39m get_calculations(all_ports[count], returns_start, returns_end)\n\u001b[1;32m     11\u001b[0m     cov_np \u001b[38;5;241m=\u001b[39m temp_cov[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCovariance\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto_numpy()\n",
      "Cell \u001b[0;32mIn[29], line 97\u001b[0m, in \u001b[0;36mcreate_portfolio\u001b[0;34m(size)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(sm_idxs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m: \n\u001b[1;32m     96\u001b[0m     replaced \u001b[38;5;241m=\u001b[39m find_least_imp(portfolio, lg_idxs, sm_idxs)\n\u001b[0;32m---> 97\u001b[0m     cur \u001b[38;5;241m=\u001b[39m ordered_info_df\u001b[38;5;241m.\u001b[39miloc[i]\n\u001b[1;32m     98\u001b[0m     cur_sector \u001b[38;5;241m=\u001b[39m cur[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSector\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    100\u001b[0m     temp_sectors \u001b[38;5;241m=\u001b[39m port_sectors\u001b[38;5;241m.\u001b[39mcopy()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/pandas/core/indexing.py:1191\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1189\u001b[0m maybe_callable \u001b[38;5;241m=\u001b[39m com\u001b[38;5;241m.\u001b[39mapply_if_callable(key, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj)\n\u001b[1;32m   1190\u001b[0m maybe_callable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_deprecated_callable_usage(key, maybe_callable)\n\u001b[0;32m-> 1191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_axis(maybe_callable, axis\u001b[38;5;241m=\u001b[39maxis)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/pandas/core/indexing.py:1752\u001b[0m, in \u001b[0;36m_iLocIndexer._getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot index by location index with a non-integer key\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1751\u001b[0m \u001b[38;5;66;03m# validate the location\u001b[39;00m\n\u001b[0;32m-> 1752\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_integer(key, axis)\n\u001b[1;32m   1754\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_ixs(key, axis\u001b[38;5;241m=\u001b[39maxis)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/pandas/core/indexing.py:1685\u001b[0m, in \u001b[0;36m_iLocIndexer._validate_integer\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1683\u001b[0m len_axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_get_axis(axis))\n\u001b[1;32m   1684\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m len_axis \u001b[38;5;129;01mor\u001b[39;00m key \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m-\u001b[39mlen_axis:\n\u001b[0;32m-> 1685\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msingle positional indexer is out-of-bounds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mIndexError\u001b[0m: single positional indexer is out-of-bounds"
     ]
    }
   ],
   "source": [
    "# Code that creates the portfolio of 10-25, and then sees which one is the most optimal \n",
    "\n",
    "all_ports = []\n",
    "all_variance = []\n",
    "all_weights = []\n",
    "count = 0\n",
    "while (count + 10) < 26:\n",
    "    all_ports.append(create_portfolio(count+10))\n",
    "\n",
    "    temp_cov = get_calculations(all_ports[count], returns_start, returns_end)\n",
    "    cov_np = temp_cov['Covariance'].to_numpy()\n",
    "    temp_var, temp_weights = secondary_minimization(cov_np)\n",
    "\n",
    "    all_variance.append(temp_var)\n",
    "    all_weights.append(temp_weights)\n",
    "    count += 1\n",
    "\n",
    "smallest_var = min(all_variance)\n",
    "index = all_variance.index(smallest_var)\n",
    "target = [smallest_var, all_ports[index], all_weights[index]]\n",
    "print(f\"The smallest variance found is {target[0]} which is determined from the following portfolio: {target[1]}, at the following weights {target[2]}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = pd.DataFrame({\n",
    "    \"Ticker\": target[1],\n",
    "    \"Weight\": target[2]\n",
    "})\n",
    "\n",
    "def get_close_prices_and_rate(tickers, target_date, end_date):\n",
    "    \"\"\"\n",
    "    :param tickers: list of tickers\n",
    "    :param target_data: the day of price we want, normally most recent business day\n",
    "    :param end_date: the day after, as yfinance is not inclusive\n",
    "    :return: a Series that contains the target days close price\n",
    "    :return: the USD to CAD exchange rate\n",
    "    \"\"\"\n",
    "    data = yf.download(tickers, start=target_date, end=end_date)[\"Close\"]\n",
    "    close_prices = data.iloc[0]\n",
    "\n",
    "    exchange_rate = yf.download(\"CAD=X\", start=target_date, end=end_date)[\"Close\"]\n",
    "    exchange_rate = exchange_rate.iloc[0]\n",
    "    return close_prices, exchange_rate.item()\n",
    "\n",
    "def purchase_flat_fee(df, close_prices, budget, exchange_rate):\n",
    "    temp_df[\"Price\"] = close_prices\n",
    "\n",
    "    df[\"Budget Flat Fee\"] = df[\"Weight\"] * (budget - (2.5*exchange_rate))\n",
    "    df[\"Shares Flat Fee\"] = df[\"Budget Flat Fee\"] / df[\"Price\"]\n",
    "\n",
    "    return df \n",
    "\n",
    "def purchase_variable_fee(df, budget, exchange_rate):\n",
    "    df[\"Shares w/o Fee\"] = (df[\"Weight\"] * budget) / df[\"Price\"]\n",
    "    total_shares = df[\"Shares w/o Fee\"].sum()\n",
    "    \n",
    "    variable_fee_usd = total_shares * 0.001\n",
    "    variable_fee_cad = variable_fee_usd * exchange_rate\n",
    "\n",
    "    adjusted_budget = budget - variable_fee_cad\n",
    "    df[\"Shares Variable Fee\"] = (df[\"Weight\"] * adjusted_budget) / df[\"Price\"]\n",
    "\n",
    "    return df\n",
    "\n",
    "closing, usd_cad_rate = get_close_prices_and_rate(target[1], target_date, end_date)\n",
    "\n",
    "def ideal_shares(df):\n",
    "    sum_flat = df[\"Shares Flat Fee\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final database returning code\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yello\\AppData\\Local\\Temp\\ipykernel_28548\\1569731745.py:1: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
      "  exchange_rate = yf.download(\"CAD=X\", start=\"2025-11-18\", end=\"2025-11-19\")[\"Close\"]\n",
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.4050500392913818"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exchange_rate = yf.download(\"CAD=X\", start=\"2025-11-18\", end=\"2025-11-19\")[\"Close\"]\n",
    "exchange_rate = exchange_rate.iloc[0]\n",
    "(exchange_rate.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contribution Declaration\n",
    "\n",
    "The following team members made a meaningful contribution to this assignment:\n",
    "\n",
    "Insert Names Here. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
